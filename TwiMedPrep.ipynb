{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corp: \n",
    "    def __init__(self, anotations, text):\n",
    "        self.anotations = anotations\n",
    "        self.text = text\n",
    "\n",
    "    def anotations(self):\n",
    "        return self.anotations\n",
    "\n",
    "    def set_anotations(self, anotations):\n",
    "        self.anotations = anotations\n",
    "    \n",
    "    def text(self):\n",
    "        return self.text\n",
    "\n",
    "    def set_text(self, text):\n",
    "        self.text = text       \n",
    "\n",
    "    def description(self):\n",
    "        result = self.text\n",
    "        for a in self.anotations:\n",
    "            result = f\"{result}\\n{a.description()}\"\n",
    "        return result\n",
    "    \n",
    "class Anotation:\n",
    "    def __init__(self, label, keyword):\n",
    "        self.label = label\n",
    "        self.keyword = keyword\n",
    "    \n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label       \n",
    "\n",
    "    def keyword(self):\n",
    "        return self.keyword\n",
    "\n",
    "    def set_keyword(self, keyword):\n",
    "        self.keyword = keyword       \n",
    "        \n",
    "    def description(self):\n",
    "        return f\"{self.label}: {self.keyword}\"\n",
    "    \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "        \n",
    "def processDir(corps_dir):\n",
    "    corps = []\n",
    "    files = listdir(corps_dir)\n",
    "    count = len(files)\n",
    "    for i, f in enumerate(files):\n",
    "        p = join(corps_dir, f)\n",
    "        if(i % 100 == 0):\n",
    "            print(f\"{i+1}/{count}\")\n",
    "        if (isfile(p)):\n",
    "            c = processFile(p)\n",
    "            if(c != None):\n",
    "                corps.append(c)\n",
    "    return corps\n",
    "\n",
    "def processFile(textfile):\n",
    "    if(textfile.endswith(\".txt\") == True):\n",
    "        annotationfile = textfile.split(\".txt\")[0] + \".ann\"\n",
    "        corp = Corp([], '')\n",
    "        lines = open(annotationfile, \"r\")\n",
    "        for line in lines: \n",
    "            a = processEntity(line)\n",
    "            if(a != None):\n",
    "                corp.anotations.append(a)\n",
    "        if isfile(textfile):\n",
    "            txt = open(textfile, \"r\").read()\n",
    "            corp.text = txt.replace(\"\\n\",\"\", 100).replace(\"__number__\", str(random.randint(1,101)))\n",
    "        return corp\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "\n",
    "import random\n",
    "      \n",
    "def processEntity(line):\n",
    "    if(line.startswith(\"T\")):\n",
    "        values = line.split(\"\\t\")\n",
    "        if len(values) > 2:\n",
    "            a = Anotation('','')\n",
    "            temp = values[1].split(\" \")\n",
    "            a.label = temp[0]\n",
    "            a.keyword = values[2].replace(\"\\n\",\"\", 100)\n",
    "            return a\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter(tweet_file, output_dir):\n",
    "    lines = open(tweet_file, \"r\")\n",
    "    for line in lines: \n",
    "        values = line.split(\"\\t\")\n",
    "        if(len(values) > 1):\n",
    "            tweet_id = values[0]\n",
    "            tweet_text = ''.join(values[1:])\n",
    "            out = open(join(output_dir, f\"{tweet_id}.txt\"), \"w\")\n",
    "            out.write(tweet_text)\n",
    "output_dir = \"./data/TwiMed/gold_conflated/twitter\"\n",
    "tweet_file = \"./data/TwiMed/tweets_ID.txt\"\n",
    "twitter(tweet_file, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import os\n",
    "from jsonCorps2conll03 import mkdir\n",
    "\n",
    "def buildCorp(rowIndex, count, corp, pipeName, nlp, out):\n",
    "    # use spacy entity ruler to generate the bootstrap corps\n",
    "    ruler = EntityRuler(nlp)\n",
    "    patterns = []\n",
    "    for a in corp.anotations:\n",
    "        word_list = []\n",
    "        for w in a.keyword.split(\" \"):\n",
    "            word_list.append({\"lower\": w.lower()})\n",
    "        patterns.append({\"label\": a.label, \"pattern\": word_list})\n",
    "    ruler.add_patterns(patterns)\n",
    "        \n",
    "    nlp.replace_pipe(pipeName, ruler)\n",
    "    if(rowIndex % 100 ==0):\n",
    "        print(f\"{rowIndex+1}/{count}\")\n",
    "    doc = nlp(corp.text)\n",
    "    out.write(\"-DOCSTART- -X- - O\\n\\n\")\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # ignore 3rd argument as \"-\" since alot of library seems to ignore the chunk tag\n",
    "            if(token.ent_type_):\n",
    "                out.write(f\"{token.orth_} {token.tag_} - {token.ent_iob_}-{token.ent_type_}\\n\")\n",
    "            else:\n",
    "                out.write(f\"{token.orth_} {token.pos_} - O\\n\")\n",
    "        out.write(\"\\n\")\n",
    "            \n",
    "def build_conll_03(corps, outfile):\n",
    "    print(f\"writing {outfile} ...\")\n",
    "    ruler_name= 'custom'\n",
    "    nlp = spacy.load('en')\n",
    "    ruler = EntityRuler(nlp)\n",
    "    nlp.add_pipe(ruler, name=ruler_name)\n",
    "    out = open(outfile, \"w\")\n",
    "    # write start tag for conll_03 format\n",
    "    count = len(corps)\n",
    "    for i, corp in enumerate(corps):\n",
    "        buildCorp(i, count, corp, ruler_name, nlp, out)\n",
    "    out.close()\n",
    "    return count\n",
    "\n",
    "def convertTwiMedConll_03(corps, basedir, corpsname):\n",
    "    mkdir(basedir)\n",
    "    mkdir (basedir + \"/conll_03\")\n",
    "    outputname = basedir + '/conll_03/' + corpsname\n",
    "    print(outputname)   \n",
    "    build_conll_03(corps, outputname)\n",
    "    return outputname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2001\n",
      "101/2001\n",
      "201/2001\n",
      "301/2001\n",
      "401/2001\n",
      "501/2001\n",
      "601/2001\n",
      "701/2001\n",
      "801/2001\n",
      "901/2001\n",
      "1001/2001\n",
      "1101/2001\n",
      "1201/2001\n",
      "1301/2001\n",
      "1401/2001\n",
      "1501/2001\n",
      "1601/2001\n",
      "1701/2001\n",
      "1801/2001\n",
      "1901/2001\n",
      "2001/2001\n",
      "1/1608\n",
      "101/1608\n",
      "201/1608\n",
      "301/1608\n",
      "401/1608\n",
      "501/1608\n",
      "601/1608\n",
      "701/1608\n",
      "801/1608\n",
      "901/1608\n",
      "1001/1608\n",
      "1101/1608\n",
      "1201/1608\n",
      "1301/1608\n",
      "1401/1608\n",
      "1501/1608\n",
      "1601/1608\n"
     ]
    }
   ],
   "source": [
    "pmc_corps = processDir(\"./data/TwiMed/gold_conflated/pubmed\")\n",
    "twitter_corps = processDir(\"./data/TwiMed/gold_conflated/twitter\")\n",
    "corps = twitter_corps + pmc_corps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split((corps),(corps), test_size=0.4, random_state=42)\n",
    "testa, testb, x, y = train_test_split((test),(test), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC 1000\n",
      "Twitter 607\n",
      "Train : 964\n",
      "Testa : 321\n",
      "Testb : 322\n"
     ]
    }
   ],
   "source": [
    "print(f\"PMC {len(pmc_corps)}\")\n",
    "print(f\"Twitter {len(twitter_corps)}\")\n",
    "\n",
    "print(f\"Train : {len(train)}\")\n",
    "print(f\"Testa : {len(testa)}\")\n",
    "print(f\"Testb : {len(testb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twimed_conll_03/conll_03/eng.testa\n",
      "writing ./twimed_conll_03/conll_03/eng.testa ...\n",
      "1/321\n",
      "101/321\n",
      "201/321\n",
      "301/321\n",
      "./twimed_conll_03/conll_03/eng.testb\n",
      "writing ./twimed_conll_03/conll_03/eng.testb ...\n",
      "1/322\n",
      "101/322\n",
      "201/322\n",
      "301/322\n",
      "./twimed_conll_03/conll_03/eng.train\n",
      "writing ./twimed_conll_03/conll_03/eng.train ...\n",
      "1/964\n",
      "101/964\n",
      "201/964\n",
      "301/964\n",
      "401/964\n",
      "501/964\n",
      "601/964\n",
      "701/964\n",
      "801/964\n",
      "901/964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./twimed_conll_03/conll_03/eng.train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertTwiMedConll_03(testa, \"./twimed_conll_03\", \"eng.testa\")\n",
    "convertTwiMedConll_03(testb, \"./twimed_conll_03\", \"eng.testb\")\n",
    "convertTwiMedConll_03(train, \"./twimed_conll_03\", \"eng.train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 17:56:26,460 Reading data from twimed_conll_03/conll_03\n",
      "2019-03-18 17:56:26,460 Train: twimed_conll_03/conll_03/eng.train\n",
      "2019-03-18 17:56:26,460 Dev: twimed_conll_03/conll_03/eng.testa\n",
      "2019-03-18 17:56:26,461 Test: twimed_conll_03/conll_03/eng.testb\n",
      "[b'<unk>', b'O', b'S-PERSON', b'S-Drug', b'B-CARDINAL', b'I-CARDINAL', b'E-CARDINAL', b'B-Disease_Symptom', b'E-Disease_Symptom', b'I-Disease_Symptom', b'S-ORG', b'S-Disease_Symptom', b'B-DATE', b'I-DATE', b'E-DATE', b'S-CARDINAL', b'B-ORG', b'I-ORG', b'E-ORG', b'S-NORP', b'B-NORP', b'E-NORP', b'S-MONEY', b'S-GPE', b'S-DATE', b'B-TIME', b'E-TIME', b'B-PERSON', b'E-PERSON', b'B-PERCENT', b'E-PERCENT', b'B-MONEY', b'E-MONEY', b'S-ORDINAL', b'B-GPE', b'E-GPE', b'S-PRODUCT', b'I-PERCENT', b'B-Drug', b'E-Drug', b'I-PERSON', b'S-TIME', b'B-QUANTITY', b'E-QUANTITY', b'I-TIME', b'B-LAW', b'E-LAW', b'B-FAC', b'I-FAC', b'E-FAC', b'B-LOC', b'E-LOC', b'S-LOC', b'I-QUANTITY', b'B-PRODUCT', b'E-PRODUCT', b'I-GPE', b'B-WORK_OF_ART', b'I-WORK_OF_ART', b'E-WORK_OF_ART', b'S-LANGUAGE', b'I-LAW', b'I-MONEY', b'B-EVENT', b'I-EVENT', b'E-EVENT', b'<START>', b'<STOP>']\n",
      "2019-03-18 17:56:30,146 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:30,147 Evaluation method: MICRO_F1_SCORE\n",
      "2019-03-18 17:56:30,154 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:31,895 epoch 1 - iter 0/11 - loss 44.17560577\n",
      "2019-03-18 17:56:33,836 epoch 1 - iter 1/11 - loss 40.50722885\n",
      "2019-03-18 17:56:35,826 epoch 1 - iter 2/11 - loss 34.55317688\n",
      "2019-03-18 17:56:38,144 epoch 1 - iter 3/11 - loss 30.59306335\n",
      "2019-03-18 17:56:39,775 epoch 1 - iter 4/11 - loss 27.96236229\n",
      "2019-03-18 17:56:42,063 epoch 1 - iter 5/11 - loss 26.18632952\n",
      "2019-03-18 17:56:43,795 epoch 1 - iter 6/11 - loss 24.40803814\n",
      "2019-03-18 17:56:45,498 epoch 1 - iter 7/11 - loss 23.13035786\n",
      "2019-03-18 17:56:47,117 epoch 1 - iter 8/11 - loss 22.29402754\n",
      "2019-03-18 17:56:49,733 epoch 1 - iter 9/11 - loss 21.61248178\n",
      "2019-03-18 17:56:50,006 epoch 1 - iter 10/11 - loss 21.50063546\n",
      "2019-03-18 17:56:50,020 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:50,020 EPOCH 1 done: loss 21.5006 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:56:57,072 DEV  : loss 12.66107559 - f-score 0.0096 - acc 0.0048\n",
      "2019-03-18 17:57:03,573 TEST : loss 12.28554344 - f-score 0.0039 - acc 0.0019\n",
      "2019-03-18 17:57:06,238 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:06,862 epoch 2 - iter 0/11 - loss 14.20663261\n",
      "2019-03-18 17:57:07,569 epoch 2 - iter 1/11 - loss 14.04750347\n",
      "2019-03-18 17:57:08,233 epoch 2 - iter 2/11 - loss 14.07370090\n",
      "2019-03-18 17:57:08,991 epoch 2 - iter 3/11 - loss 13.71464539\n",
      "2019-03-18 17:57:09,797 epoch 2 - iter 4/11 - loss 13.86521835\n",
      "2019-03-18 17:57:10,685 epoch 2 - iter 5/11 - loss 13.76049868\n",
      "2019-03-18 17:57:11,390 epoch 2 - iter 6/11 - loss 13.47089005\n",
      "2019-03-18 17:57:12,091 epoch 2 - iter 7/11 - loss 13.10215271\n",
      "2019-03-18 17:57:13,019 epoch 2 - iter 8/11 - loss 12.92426872\n",
      "2019-03-18 17:57:13,648 epoch 2 - iter 9/11 - loss 12.68688488\n",
      "2019-03-18 17:57:13,874 epoch 2 - iter 10/11 - loss 12.73621159\n",
      "2019-03-18 17:57:13,888 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:13,889 EPOCH 2 done: loss 12.7362 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:16,675 DEV  : loss 9.32568836 - f-score 0.0205 - acc 0.0103\n",
      "2019-03-18 17:57:19,344 TEST : loss 8.97776222 - f-score 0.0128 - acc 0.0064\n",
      "2019-03-18 17:57:24,978 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:25,760 epoch 3 - iter 0/11 - loss 9.99269676\n",
      "2019-03-18 17:57:26,391 epoch 3 - iter 1/11 - loss 10.09763527\n",
      "2019-03-18 17:57:27,091 epoch 3 - iter 2/11 - loss 10.23884328\n",
      "2019-03-18 17:57:28,035 epoch 3 - iter 3/11 - loss 10.53505182\n",
      "2019-03-18 17:57:28,857 epoch 3 - iter 4/11 - loss 10.56975632\n",
      "2019-03-18 17:57:29,564 epoch 3 - iter 5/11 - loss 10.27961842\n",
      "2019-03-18 17:57:30,244 epoch 3 - iter 6/11 - loss 10.25106580\n",
      "2019-03-18 17:57:30,858 epoch 3 - iter 7/11 - loss 10.38041413\n",
      "2019-03-18 17:57:31,566 epoch 3 - iter 8/11 - loss 10.43578243\n",
      "2019-03-18 17:57:32,460 epoch 3 - iter 9/11 - loss 10.41155624\n",
      "2019-03-18 17:57:32,612 epoch 3 - iter 10/11 - loss 10.38365562\n",
      "2019-03-18 17:57:32,626 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:32,627 EPOCH 3 done: loss 10.3837 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:35,415 DEV  : loss 9.12169075 - f-score 0.0214 - acc 0.0108\n",
      "2019-03-18 17:57:38,082 TEST : loss 8.74884129 - f-score 0.0196 - acc 0.0099\n",
      "2019-03-18 17:57:43,157 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:43,907 epoch 4 - iter 0/11 - loss 8.19914436\n",
      "2019-03-18 17:57:44,621 epoch 4 - iter 1/11 - loss 8.74515247\n",
      "2019-03-18 17:57:45,561 epoch 4 - iter 2/11 - loss 9.32699362\n",
      "2019-03-18 17:57:46,455 epoch 4 - iter 3/11 - loss 9.39928699\n",
      "2019-03-18 17:57:47,283 epoch 4 - iter 4/11 - loss 9.75068874\n",
      "2019-03-18 17:57:47,994 epoch 4 - iter 5/11 - loss 9.63561455\n",
      "2019-03-18 17:57:48,588 epoch 4 - iter 6/11 - loss 9.54589203\n",
      "2019-03-18 17:57:49,231 epoch 4 - iter 7/11 - loss 9.49313271\n",
      "2019-03-18 17:57:49,904 epoch 4 - iter 8/11 - loss 9.52759753\n",
      "2019-03-18 17:57:50,606 epoch 4 - iter 9/11 - loss 9.49480448\n",
      "2019-03-18 17:57:50,759 epoch 4 - iter 10/11 - loss 9.46416825\n",
      "2019-03-18 17:57:50,773 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:50,774 EPOCH 4 done: loss 9.4642 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:53,569 DEV  : loss 7.39441586 - f-score 0.4120 - acc 0.2595\n",
      "2019-03-18 17:57:56,257 TEST : loss 7.06210852 - f-score 0.4233 - acc 0.2684\n",
      "2019-03-18 17:58:01,449 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:02,192 epoch 5 - iter 0/11 - loss 8.43217754\n",
      "2019-03-18 17:58:02,819 epoch 5 - iter 1/11 - loss 8.48410082\n",
      "2019-03-18 17:58:03,523 epoch 5 - iter 2/11 - loss 8.52130985\n",
      "2019-03-18 17:58:04,129 epoch 5 - iter 3/11 - loss 8.41020131\n",
      "2019-03-18 17:58:05,076 epoch 5 - iter 4/11 - loss 8.61309643\n",
      "2019-03-18 17:58:05,755 epoch 5 - iter 5/11 - loss 8.43806251\n",
      "2019-03-18 17:58:06,443 epoch 5 - iter 6/11 - loss 8.52567823\n",
      "2019-03-18 17:58:07,337 epoch 5 - iter 7/11 - loss 8.54056919\n",
      "2019-03-18 17:58:08,152 epoch 5 - iter 8/11 - loss 8.51146412\n",
      "2019-03-18 17:58:08,781 epoch 5 - iter 9/11 - loss 8.47442732\n",
      "2019-03-18 17:58:09,069 epoch 5 - iter 10/11 - loss 8.50992989\n",
      "2019-03-18 17:58:09,083 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:09,083 EPOCH 5 done: loss 8.5099 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:11,879 DEV  : loss 7.08802319 - f-score 0.3310 - acc 0.1983\n",
      "2019-03-18 17:58:14,563 TEST : loss 6.71771431 - f-score 0.3113 - acc 0.1844\n",
      "2019-03-18 17:58:19,736 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:20,426 epoch 6 - iter 0/11 - loss 7.92453337\n",
      "2019-03-18 17:58:21,471 epoch 6 - iter 1/11 - loss 8.77775979\n",
      "2019-03-18 17:58:22,421 epoch 6 - iter 2/11 - loss 8.47536310\n",
      "2019-03-18 17:58:23,036 epoch 6 - iter 3/11 - loss 8.39170980\n",
      "2019-03-18 17:58:23,638 epoch 6 - iter 4/11 - loss 8.32213879\n",
      "2019-03-18 17:58:24,276 epoch 6 - iter 5/11 - loss 8.08560864\n",
      "2019-03-18 17:58:25,090 epoch 6 - iter 6/11 - loss 8.14814935\n",
      "2019-03-18 17:58:25,785 epoch 6 - iter 7/11 - loss 8.01995218\n",
      "2019-03-18 17:58:26,409 epoch 6 - iter 8/11 - loss 7.84352154\n",
      "2019-03-18 17:58:27,107 epoch 6 - iter 9/11 - loss 7.88991308\n",
      "2019-03-18 17:58:27,259 epoch 6 - iter 10/11 - loss 7.89073251\n",
      "2019-03-18 17:58:27,274 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:27,274 EPOCH 6 done: loss 7.8907 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:30,057 DEV  : loss 6.11837435 - f-score 0.4590 - acc 0.2978\n",
      "2019-03-18 17:58:32,721 TEST : loss 5.77262497 - f-score 0.4644 - acc 0.3025\n",
      "2019-03-18 17:58:37,666 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:38,281 epoch 7 - iter 0/11 - loss 6.67020273\n",
      "2019-03-18 17:58:39,101 epoch 7 - iter 1/11 - loss 7.34987855\n",
      "2019-03-18 17:58:39,703 epoch 7 - iter 2/11 - loss 7.15557591\n",
      "2019-03-18 17:58:40,400 epoch 7 - iter 3/11 - loss 6.96355498\n",
      "2019-03-18 17:58:41,010 epoch 7 - iter 4/11 - loss 6.84079437\n",
      "2019-03-18 17:58:41,639 epoch 7 - iter 5/11 - loss 6.92933838\n",
      "2019-03-18 17:58:42,411 epoch 7 - iter 6/11 - loss 7.21978371\n",
      "2019-03-18 17:58:43,097 epoch 7 - iter 7/11 - loss 7.23629302\n",
      "2019-03-18 17:58:43,979 epoch 7 - iter 8/11 - loss 7.21164571\n",
      "2019-03-18 17:58:44,934 epoch 7 - iter 9/11 - loss 7.34780402\n",
      "2019-03-18 17:58:45,119 epoch 7 - iter 10/11 - loss 7.32805607\n",
      "2019-03-18 17:58:45,133 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:45,133 EPOCH 7 done: loss 7.3281 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:48,071 DEV  : loss 5.93869591 - f-score 0.3889 - acc 0.2414\n",
      "2019-03-18 17:58:50,739 TEST : loss 5.58505630 - f-score 0.3908 - acc 0.2429\n",
      "2019-03-18 17:58:56,095 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:56,798 epoch 8 - iter 0/11 - loss 5.46142101\n",
      "2019-03-18 17:58:57,439 epoch 8 - iter 1/11 - loss 5.89010191\n",
      "2019-03-18 17:58:58,174 epoch 8 - iter 2/11 - loss 6.34718418\n",
      "2019-03-18 17:58:58,924 epoch 8 - iter 3/11 - loss 6.30638206\n",
      "2019-03-18 17:58:59,733 epoch 8 - iter 4/11 - loss 6.28721876\n",
      "2019-03-18 17:59:00,435 epoch 8 - iter 5/11 - loss 6.44984404\n",
      "2019-03-18 17:59:01,053 epoch 8 - iter 6/11 - loss 6.61887836\n",
      "2019-03-18 17:59:01,721 epoch 8 - iter 7/11 - loss 6.41014194\n",
      "2019-03-18 17:59:02,678 epoch 8 - iter 8/11 - loss 6.64705234\n",
      "2019-03-18 17:59:03,588 epoch 8 - iter 9/11 - loss 6.74860792\n",
      "2019-03-18 17:59:03,799 epoch 8 - iter 10/11 - loss 6.74168421\n",
      "2019-03-18 17:59:03,814 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:03,814 EPOCH 8 done: loss 6.7417 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:06,638 DEV  : loss 5.36948490 - f-score 0.4989 - acc 0.3324\n",
      "2019-03-18 17:59:09,328 TEST : loss 5.03092718 - f-score 0.5176 - acc 0.3492\n",
      "2019-03-18 17:59:14,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:15,344 epoch 9 - iter 0/11 - loss 7.17939377\n",
      "2019-03-18 17:59:16,056 epoch 9 - iter 1/11 - loss 7.15201283\n",
      "2019-03-18 17:59:16,761 epoch 9 - iter 2/11 - loss 7.26948770\n",
      "2019-03-18 17:59:17,697 epoch 9 - iter 3/11 - loss 7.16156352\n",
      "2019-03-18 17:59:18,397 epoch 9 - iter 4/11 - loss 6.71659307\n",
      "2019-03-18 17:59:19,039 epoch 9 - iter 5/11 - loss 6.43846965\n",
      "2019-03-18 17:59:19,940 epoch 9 - iter 6/11 - loss 6.41128308\n",
      "2019-03-18 17:59:20,546 epoch 9 - iter 7/11 - loss 6.21924782\n",
      "2019-03-18 17:59:21,203 epoch 9 - iter 8/11 - loss 6.23941856\n",
      "2019-03-18 17:59:21,969 epoch 9 - iter 9/11 - loss 6.31300049\n",
      "2019-03-18 17:59:22,130 epoch 9 - iter 10/11 - loss 6.27388543\n",
      "2019-03-18 17:59:22,143 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:22,144 EPOCH 9 done: loss 6.2739 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:24,924 DEV  : loss 5.07834387 - f-score 0.4987 - acc 0.3321\n",
      "2019-03-18 17:59:27,595 TEST : loss 4.70623636 - f-score 0.5009 - acc 0.3342\n",
      "2019-03-18 17:59:32,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:33,737 epoch 10 - iter 0/11 - loss 6.25418997\n",
      "2019-03-18 17:59:34,438 epoch 10 - iter 1/11 - loss 5.71040273\n",
      "2019-03-18 17:59:35,148 epoch 10 - iter 2/11 - loss 5.84132926\n",
      "2019-03-18 17:59:36,083 epoch 10 - iter 3/11 - loss 5.89833522\n",
      "2019-03-18 17:59:36,723 epoch 10 - iter 4/11 - loss 5.79887238\n",
      "2019-03-18 17:59:37,403 epoch 10 - iter 5/11 - loss 5.74698464\n",
      "2019-03-18 17:59:38,115 epoch 10 - iter 6/11 - loss 5.78182772\n",
      "2019-03-18 17:59:39,033 epoch 10 - iter 7/11 - loss 5.96230614\n",
      "2019-03-18 17:59:39,734 epoch 10 - iter 8/11 - loss 5.90691784\n",
      "2019-03-18 17:59:40,370 epoch 10 - iter 9/11 - loss 5.84485650\n",
      "2019-03-18 17:59:40,615 epoch 10 - iter 10/11 - loss 5.85160390\n",
      "2019-03-18 17:59:40,629 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:40,629 EPOCH 10 done: loss 5.8516 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:43,441 DEV  : loss 4.95747089 - f-score 0.4591 - acc 0.2980\n",
      "2019-03-18 17:59:46,129 TEST : loss 4.59088039 - f-score 0.4632 - acc 0.3014\n",
      "2019-03-18 17:59:51,124 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:51,654 epoch 11 - iter 0/11 - loss 5.71006632\n",
      "2019-03-18 17:59:52,332 epoch 11 - iter 1/11 - loss 5.64222813\n",
      "2019-03-18 17:59:53,221 epoch 11 - iter 2/11 - loss 5.55693849\n",
      "2019-03-18 17:59:53,976 epoch 11 - iter 3/11 - loss 5.84419525\n",
      "2019-03-18 17:59:54,684 epoch 11 - iter 4/11 - loss 5.79595842\n",
      "2019-03-18 17:59:55,295 epoch 11 - iter 5/11 - loss 5.84953348\n",
      "2019-03-18 17:59:55,904 epoch 11 - iter 6/11 - loss 5.78511463\n",
      "2019-03-18 17:59:56,722 epoch 11 - iter 7/11 - loss 5.74038023\n",
      "2019-03-18 17:59:57,354 epoch 11 - iter 8/11 - loss 5.67391464\n",
      "2019-03-18 17:59:57,960 epoch 11 - iter 9/11 - loss 5.67721190\n",
      "2019-03-18 17:59:58,401 epoch 11 - iter 10/11 - loss 5.69079367\n",
      "2019-03-18 17:59:58,414 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:58,415 EPOCH 11 done: loss 5.6908 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:01,228 DEV  : loss 4.66452551 - f-score 0.5329 - acc 0.3632\n",
      "2019-03-18 18:00:03,916 TEST : loss 4.25875139 - f-score 0.5418 - acc 0.3716\n",
      "2019-03-18 18:00:09,743 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:10,468 epoch 12 - iter 0/11 - loss 5.96336031\n",
      "2019-03-18 18:00:11,113 epoch 12 - iter 1/11 - loss 5.62485313\n",
      "2019-03-18 18:00:11,707 epoch 12 - iter 2/11 - loss 5.42428382\n",
      "2019-03-18 18:00:12,408 epoch 12 - iter 3/11 - loss 5.54216826\n",
      "2019-03-18 18:00:13,034 epoch 12 - iter 4/11 - loss 5.41737337\n",
      "2019-03-18 18:00:13,700 epoch 12 - iter 5/11 - loss 5.32737501\n",
      "2019-03-18 18:00:14,599 epoch 12 - iter 6/11 - loss 5.41365726\n",
      "2019-03-18 18:00:15,534 epoch 12 - iter 7/11 - loss 5.29460144\n",
      "2019-03-18 18:00:16,354 epoch 12 - iter 8/11 - loss 5.25848685\n",
      "2019-03-18 18:00:17,073 epoch 12 - iter 9/11 - loss 5.39782934\n",
      "2019-03-18 18:00:17,241 epoch 12 - iter 10/11 - loss 5.39045261\n",
      "2019-03-18 18:00:17,254 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:17,255 EPOCH 12 done: loss 5.3905 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:20,065 DEV  : loss 4.43521547 - f-score 0.5669 - acc 0.3955\n",
      "2019-03-18 18:00:22,754 TEST : loss 4.09914017 - f-score 0.5771 - acc 0.4057\n",
      "2019-03-18 18:00:28,016 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:28,649 epoch 13 - iter 0/11 - loss 4.60199356\n",
      "2019-03-18 18:00:29,596 epoch 13 - iter 1/11 - loss 5.16084981\n",
      "2019-03-18 18:00:30,399 epoch 13 - iter 2/11 - loss 5.16295862\n",
      "2019-03-18 18:00:31,033 epoch 13 - iter 3/11 - loss 5.25921547\n",
      "2019-03-18 18:00:31,787 epoch 13 - iter 4/11 - loss 5.21205482\n",
      "2019-03-18 18:00:32,490 epoch 13 - iter 5/11 - loss 5.19422070\n",
      "2019-03-18 18:00:33,142 epoch 13 - iter 6/11 - loss 5.34494202\n",
      "2019-03-18 18:00:34,038 epoch 13 - iter 7/11 - loss 5.21362990\n",
      "2019-03-18 18:00:34,748 epoch 13 - iter 8/11 - loss 5.26232640\n",
      "2019-03-18 18:00:35,360 epoch 13 - iter 9/11 - loss 5.22053423\n",
      "2019-03-18 18:00:35,653 epoch 13 - iter 10/11 - loss 5.21047910\n",
      "2019-03-18 18:00:35,667 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:35,667 EPOCH 13 done: loss 5.2105 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:38,485 DEV  : loss 4.34682655 - f-score 0.5686 - acc 0.3972\n",
      "2019-03-18 18:00:41,187 TEST : loss 4.02324915 - f-score 0.5768 - acc 0.4053\n",
      "2019-03-18 18:00:46,419 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:47,079 epoch 14 - iter 0/11 - loss 4.25793743\n",
      "2019-03-18 18:00:47,799 epoch 14 - iter 1/11 - loss 4.46407008\n",
      "2019-03-18 18:00:48,435 epoch 14 - iter 2/11 - loss 4.47958120\n",
      "2019-03-18 18:00:49,045 epoch 14 - iter 3/11 - loss 4.58879018\n",
      "2019-03-18 18:00:49,997 epoch 14 - iter 4/11 - loss 4.89532642\n",
      "2019-03-18 18:00:50,715 epoch 14 - iter 5/11 - loss 4.92798138\n",
      "2019-03-18 18:00:51,359 epoch 14 - iter 6/11 - loss 4.87303584\n",
      "2019-03-18 18:00:52,121 epoch 14 - iter 7/11 - loss 4.85738635\n",
      "2019-03-18 18:00:52,800 epoch 14 - iter 8/11 - loss 5.00722196\n",
      "2019-03-18 18:00:53,604 epoch 14 - iter 9/11 - loss 4.87391279\n",
      "2019-03-18 18:00:53,785 epoch 14 - iter 10/11 - loss 4.90385286\n",
      "2019-03-18 18:00:53,799 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:53,799 EPOCH 14 done: loss 4.9039 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:56,608 DEV  : loss 4.20933580 - f-score 0.5594 - acc 0.3883\n",
      "2019-03-18 18:00:59,297 TEST : loss 3.84771705 - f-score 0.5717 - acc 0.4003\n",
      "2019-03-18 18:01:04,307 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:05,013 epoch 15 - iter 0/11 - loss 4.30351925\n",
      "2019-03-18 18:01:05,666 epoch 15 - iter 1/11 - loss 4.59920287\n",
      "2019-03-18 18:01:06,197 epoch 15 - iter 2/11 - loss 4.40035756\n",
      "2019-03-18 18:01:06,870 epoch 15 - iter 3/11 - loss 4.69872618\n",
      "2019-03-18 18:01:07,577 epoch 15 - iter 4/11 - loss 4.61874752\n",
      "2019-03-18 18:01:08,514 epoch 15 - iter 5/11 - loss 4.49880425\n",
      "2019-03-18 18:01:09,334 epoch 15 - iter 6/11 - loss 4.59480470\n",
      "2019-03-18 18:01:10,041 epoch 15 - iter 7/11 - loss 4.65069973\n",
      "2019-03-18 18:01:10,639 epoch 15 - iter 8/11 - loss 4.67297787\n",
      "2019-03-18 18:01:11,538 epoch 15 - iter 9/11 - loss 4.68370848\n",
      "2019-03-18 18:01:11,736 epoch 15 - iter 10/11 - loss 4.70204913\n",
      "2019-03-18 18:01:11,750 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:11,750 EPOCH 15 done: loss 4.7020 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:14,561 DEV  : loss 4.16310740 - f-score 0.5905 - acc 0.4190\n",
      "2019-03-18 18:01:17,255 TEST : loss 3.84660769 - f-score 0.6050 - acc 0.4337\n",
      "2019-03-18 18:01:22,230 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:22,862 epoch 16 - iter 0/11 - loss 3.54200029\n",
      "2019-03-18 18:01:23,628 epoch 16 - iter 1/11 - loss 4.15624690\n",
      "2019-03-18 18:01:24,324 epoch 16 - iter 2/11 - loss 4.29155906\n",
      "2019-03-18 18:01:25,278 epoch 16 - iter 3/11 - loss 4.83133113\n",
      "2019-03-18 18:01:25,958 epoch 16 - iter 4/11 - loss 4.83491411\n",
      "2019-03-18 18:01:26,583 epoch 16 - iter 5/11 - loss 4.76907198\n",
      "2019-03-18 18:01:27,482 epoch 16 - iter 6/11 - loss 4.87205798\n",
      "2019-03-18 18:01:28,197 epoch 16 - iter 7/11 - loss 4.81924462\n",
      "2019-03-18 18:01:28,752 epoch 16 - iter 8/11 - loss 4.69889336\n",
      "2019-03-18 18:01:29,454 epoch 16 - iter 9/11 - loss 4.63282320\n",
      "2019-03-18 18:01:29,643 epoch 16 - iter 10/11 - loss 4.62375697\n",
      "2019-03-18 18:01:29,657 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:29,657 EPOCH 16 done: loss 4.6238 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:32,448 DEV  : loss 3.99411488 - f-score 0.5680 - acc 0.3966\n",
      "2019-03-18 18:01:35,124 TEST : loss 3.64926696 - f-score 0.5800 - acc 0.4085\n",
      "2019-03-18 18:01:40,335 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:40,992 epoch 17 - iter 0/11 - loss 4.51284742\n",
      "2019-03-18 18:01:41,805 epoch 17 - iter 1/11 - loss 4.77409673\n",
      "2019-03-18 18:01:42,486 epoch 17 - iter 2/11 - loss 4.55030553\n",
      "2019-03-18 18:01:43,135 epoch 17 - iter 3/11 - loss 4.58373928\n",
      "2019-03-18 18:01:43,891 epoch 17 - iter 4/11 - loss 4.58858814\n",
      "2019-03-18 18:01:44,523 epoch 17 - iter 5/11 - loss 4.63158258\n",
      "2019-03-18 18:01:45,456 epoch 17 - iter 6/11 - loss 4.43642787\n",
      "2019-03-18 18:01:46,168 epoch 17 - iter 7/11 - loss 4.51382011\n",
      "2019-03-18 18:01:47,062 epoch 17 - iter 8/11 - loss 4.54163424\n",
      "2019-03-18 18:01:47,781 epoch 17 - iter 9/11 - loss 4.54151521\n",
      "2019-03-18 18:01:47,937 epoch 17 - iter 10/11 - loss 4.53877623\n",
      "2019-03-18 18:01:47,953 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:47,953 EPOCH 17 done: loss 4.5388 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:50,766 DEV  : loss 3.85810399 - f-score 0.5922 - acc 0.4206\n",
      "2019-03-18 18:01:53,455 TEST : loss 3.54655409 - f-score 0.6145 - acc 0.4435\n",
      "2019-03-18 18:01:58,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:59,345 epoch 18 - iter 0/11 - loss 4.32796431\n",
      "2019-03-18 18:01:59,955 epoch 18 - iter 1/11 - loss 4.73040605\n",
      "2019-03-18 18:02:00,718 epoch 18 - iter 2/11 - loss 4.47780776\n",
      "2019-03-18 18:02:01,431 epoch 18 - iter 3/11 - loss 4.54662472\n",
      "2019-03-18 18:02:02,388 epoch 18 - iter 4/11 - loss 4.58515563\n",
      "2019-03-18 18:02:03,072 epoch 18 - iter 5/11 - loss 4.50415059\n",
      "2019-03-18 18:02:03,788 epoch 18 - iter 6/11 - loss 4.40703028\n",
      "2019-03-18 18:02:04,681 epoch 18 - iter 7/11 - loss 4.45792547\n",
      "2019-03-18 18:02:05,490 epoch 18 - iter 8/11 - loss 4.39516436\n",
      "2019-03-18 18:02:06,139 epoch 18 - iter 9/11 - loss 4.31525629\n",
      "2019-03-18 18:02:06,336 epoch 18 - iter 10/11 - loss 4.31545827\n",
      "2019-03-18 18:02:06,349 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:02:06,350 EPOCH 18 done: loss 4.3155 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:02:09,167 DEV  : loss 3.82240725 - f-score 0.6072 - acc 0.4360\n"
     ]
    }
   ],
   "source": [
    "from jsonCorps2conll03 import rmdir\n",
    "from flairNER import train\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "conll_03_corps_folder = 'twimed_conll_03'\n",
    "model_output_folder = 'twimed-ner'\n",
    "rmdir(conll_03_corps_folder)\n",
    "train(conll_03_corps_folder, model_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 13:32:25,154 loading file twimed-ner/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "model_output_folder = 'twimed-ner'\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence(\"\"\"\n",
    "Previous studies have demonstrated that glucocorticoid hormones, including dexamethasone, induced alterations in intracellular calcium homeostasis in acute lymphoblastic leukemia (ALL) cells. However, the mechanism by which intracellular calcium homeostasis participates in dexamethasone sensitivity and resistance on ALL cells remains elusive. Here, we found that treatment of cells with dexamethasone resulted in increased intracellular calcium concentrations through store-operated calcium entry stimulation, which was curtailed by store-operated calcium channel blockers. We show that BAPTA-AM, an intracellular Ca2+ chelator, synergistically enhances dexamethasone lethality in two human ALL cell lines and in three primary specimens. This effect correlated with the inhibition of the prosurvival kinase ERK1/2 signaling pathway. Chelating intracellular calcium with Bapta-AM or inhibiting ERK1/2 with PD98059 significantly potentiated dexamethasone-induced mitochondrial membrane potential collapse, reactive oxygen species production, cytochrome c release, caspase-3 activity, and cell death. Moreover, we show that thapsigargin elevates intracellular free calcium ion level, and activates ERK1/2 signaling, resulting in the inhibition of dexamethasone-induced ALL cells apoptosis. Together, these results indicate that calcium-related ERK1/2 signaling pathway contributes to protect cells from dexamethasone sensitivity by limiting mitochondrial apoptotic pathway. This report provides a novel resistance pathway underlying the regulatory effect of dexamethasone on ALL cells.\n",
    "\"\"\")\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load_from_file(model_output_folder + '/final-model.pt')\n",
    "\n",
    "def detect(tagger, text):\n",
    "    print('===============================================')\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    print(sentence)\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # iterate over entities and print\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(entity)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"Starting back on fluoxetine tonight, I did pick up the prescription a few days ago but in the past when I’ve started/upped meds I’ve had to call in sick to work due to side effects. So I waited. Now I have 3 days off to adjust\" - 46 Tokens\n",
      "--------------------------------\n",
      "Drug-span [4]: \"fluoxetine\"\n",
      "DATE-span [13,14]: \"few days\"\n",
      "DATE-span [42,43]: \"3 days\"\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "detect(tagger, \"\"\"Starting back on fluoxetine tonight, I did pick up the prescription a few days ago but in the past when I’ve started/upped meds I’ve had to call in sick to work due to side effects. So I waited. Now I have 3 days off to adjust\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"Publication alert: combining CBT with fluoxetine might be superior to either therapy for adolescents with depression. Model-based random forest method applied in a study by @HeidiBaya Seibold, T.Hothorn, S.Foster, M.Mohler-Kuo,\" - 30 Tokens\n",
      "--------------------------------\n",
      "ORG-span [4]: \"CBT\"\n",
      "Drug-span [6]: \"fluoxetine\"\n",
      "ORG-span [26,27]: \"@HeidiBaya Seibold,\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"Publication alert: combining CBT with #fluoxetine might be superior to either therapy for adolescents with #depression. Model-based random forest method applied in a study by @HeidiBaya Seibold, T.Hothorn, S.Foster, M.Mohler-Kuo, \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"sleepless nights, feeling worthless, lifes trash, this shit aint worth it man, fluoxetine isnt doing nothing. ive come to conclusion life is fucking worthless and i wish everyone the best, fuck this, i cant handle this shit mentally anymore, fuck life, im done with life. bye\" - 46 Tokens\n",
      "--------------------------------\n",
      "Drug-span [13]: \"fluoxetine\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"sleepless nights, feeling worthless, lifes trash, this shit aint worth it man, fluoxetine isnt doing nothing. ive come to conclusion life is fucking worthless and i wish everyone the best, fuck this, i cant handle this shit mentally anymore, fuck life,  im done with life.  bye\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"i’m i’m terrified of putting anything in my body (that’s why i don’t smoke and rarely drink). i’m even scared of taking tylenol sometimes. antibiotics sound like the end of the world to me. idk. help.\" - 36 Tokens\n",
      "--------------------------------\n",
      "Disease_Symptom-span [14]: \"smoke\"\n",
      "Drug-span [23]: \"tylenol\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"i’m i’m terrified of putting anything in my body (that’s why i don’t smoke and rarely drink). i’m even scared of taking tylenol sometimes. antibiotics sound like the end of the world to me. idk. help.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
