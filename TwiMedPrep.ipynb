{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corp: \n",
    "    def __init__(self, anotations, text):\n",
    "        self.anotations = anotations\n",
    "        self.text = text\n",
    "\n",
    "    def anotations(self):\n",
    "        return self.anotations\n",
    "\n",
    "    def set_anotations(self, anotations):\n",
    "        self.anotations = anotations\n",
    "    \n",
    "    def text(self):\n",
    "        return self.text\n",
    "\n",
    "    def set_text(self, text):\n",
    "        self.text = text       \n",
    "\n",
    "    def description(self):\n",
    "        result = self.text\n",
    "        for a in self.anotations:\n",
    "            result = f\"{result}\\n{a.description()}\"\n",
    "        return result\n",
    "    \n",
    "class Anotation:\n",
    "    def __init__(self, label, keyword):\n",
    "        self.label = label\n",
    "        self.keyword = keyword\n",
    "    \n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label       \n",
    "\n",
    "    def keyword(self):\n",
    "        return self.keyword\n",
    "\n",
    "    def set_keyword(self, keyword):\n",
    "        self.keyword = keyword       \n",
    "        \n",
    "    def description(self):\n",
    "        return f\"{self.label}: {self.keyword}\"\n",
    "    \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "        \n",
    "def processDir(corps_dir):\n",
    "    corps = []\n",
    "    files = listdir(corps_dir)\n",
    "    count = len(files)\n",
    "    for i, f in enumerate(files):\n",
    "        p = join(corps_dir, f)\n",
    "        if(i % 100 == 0):\n",
    "            print(f\"{i+1}/{count}\")\n",
    "        if (isfile(p)):\n",
    "            c = processFile(p)\n",
    "            if(c != None):\n",
    "                corps.append(c)\n",
    "    return corps\n",
    "\n",
    "def processFile(textfile):\n",
    "    if(textfile.endswith(\".txt\") == True):\n",
    "        annotationfile = textfile.split(\".txt\")[0] + \".ann\"\n",
    "        corp = Corp([], '')\n",
    "        lines = open(annotationfile, \"r\")\n",
    "        for line in lines: \n",
    "            a = processEntity(line)\n",
    "            if(a != None):\n",
    "                corp.anotations.append(a)\n",
    "        if isfile(textfile):\n",
    "            txt = open(textfile, \"r\").read()\n",
    "            corp.text = txt.replace(\"\\n\",\"\", 100).replace(\"__number__\", str(random.randint(1,101)))\n",
    "        return corp\n",
    "    else:\n",
    "        return None\n",
    "            \n",
    "\n",
    "import random\n",
    "      \n",
    "def processEntity(line):\n",
    "    if(line.startswith(\"T\")):\n",
    "        values = line.split(\"\\t\")\n",
    "        if len(values) > 2:\n",
    "            a = Anotation('','')\n",
    "            temp = values[1].split(\" \")\n",
    "            a.label = temp[0]\n",
    "            a.keyword = values[2].replace(\"\\n\",\"\", 100)\n",
    "            return a\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter(tweet_file, output_dir):\n",
    "    lines = open(tweet_file, \"r\")\n",
    "    for line in lines: \n",
    "        values = line.split(\"\\t\")\n",
    "        if(len(values) > 1):\n",
    "            tweet_id = values[0]\n",
    "            tweet_text = ''.join(values[1:])\n",
    "            out = open(join(output_dir, f\"{tweet_id}.txt\"), \"w\")\n",
    "            out.write(tweet_text)\n",
    "output_dir = \"./data/TwiMed/gold_conflated/twitter\"\n",
    "tweet_file = \"./data/TwiMed/tweets_ID.txt\"\n",
    "twitter(tweet_file, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import os\n",
    "from jsonCorps2conll03 import mkdir\n",
    "\n",
    "def buildCorp(rowIndex, count, corp, pipeName, nlp, out):\n",
    "    # use spacy entity ruler to generate the bootstrap corps\n",
    "    ruler = EntityRuler(nlp)\n",
    "    patterns = []\n",
    "    for a in corp.anotations:\n",
    "        word_list = []\n",
    "        for w in a.keyword.split(\" \"):\n",
    "            word_list.append({\"lower\": w.lower()})\n",
    "        patterns.append({\"label\": a.label, \"pattern\": word_list})\n",
    "    ruler.add_patterns(patterns)\n",
    "        \n",
    "    nlp.replace_pipe(pipeName, ruler)\n",
    "    if(rowIndex % 100 ==0):\n",
    "        print(f\"{rowIndex+1}/{count}\")\n",
    "    doc = nlp(corp.text)\n",
    "    out.write(\"-DOCSTART- -X- - O\\n\\n\")\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # ignore 3rd argument as \"-\" since alot of library seems to ignore the chunk tag\n",
    "            if(token.ent_type_):\n",
    "                out.write(f\"{token.orth_} {token.tag_} - {token.ent_iob_}-{token.ent_type_}\\n\")\n",
    "            else:\n",
    "                out.write(f\"{token.orth_} {token.pos_} - O\\n\")\n",
    "        out.write(\"\\n\")\n",
    "            \n",
    "def build_conll_03(corps, outfile):\n",
    "    print(f\"writing {outfile} ...\")\n",
    "    ruler_name= 'custom'\n",
    "    nlp = spacy.load('en')\n",
    "    ruler = EntityRuler(nlp)\n",
    "    nlp.add_pipe(ruler, name=ruler_name)\n",
    "    out = open(outfile, \"w\")\n",
    "    # write start tag for conll_03 format\n",
    "    count = len(corps)\n",
    "    for i, corp in enumerate(corps):\n",
    "        buildCorp(i, count, corp, ruler_name, nlp, out)\n",
    "    out.close()\n",
    "    return count\n",
    "\n",
    "def convertTwiMedConll_03(corps, basedir, corpsname):\n",
    "    mkdir(basedir)\n",
    "    mkdir (basedir + \"/conll_03\")\n",
    "    outputname = basedir + '/conll_03/' + corpsname\n",
    "    print(outputname)   \n",
    "    build_conll_03(corps, outputname)\n",
    "    return outputname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2001\n",
      "101/2001\n",
      "201/2001\n",
      "301/2001\n",
      "401/2001\n",
      "501/2001\n",
      "601/2001\n",
      "701/2001\n",
      "801/2001\n",
      "901/2001\n",
      "1001/2001\n",
      "1101/2001\n",
      "1201/2001\n",
      "1301/2001\n",
      "1401/2001\n",
      "1501/2001\n",
      "1601/2001\n",
      "1701/2001\n",
      "1801/2001\n",
      "1901/2001\n",
      "2001/2001\n",
      "1/1608\n",
      "101/1608\n",
      "201/1608\n",
      "301/1608\n",
      "401/1608\n",
      "501/1608\n",
      "601/1608\n",
      "701/1608\n",
      "801/1608\n",
      "901/1608\n",
      "1001/1608\n",
      "1101/1608\n",
      "1201/1608\n",
      "1301/1608\n",
      "1401/1608\n",
      "1501/1608\n",
      "1601/1608\n"
     ]
    }
   ],
   "source": [
    "pmc_corps = processDir(\"./data/TwiMed/gold_conflated/pubmed\")\n",
    "twitter_corps = processDir(\"./data/TwiMed/gold_conflated/twitter\")\n",
    "corps = twitter_corps + pmc_corps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split((corps),(corps), test_size=0.4, random_state=42)\n",
    "testa, testb, x, y = train_test_split((test),(test), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMC 1000\n",
      "Twitter 607\n",
      "Train : 964\n",
      "Testa : 321\n",
      "Testb : 322\n"
     ]
    }
   ],
   "source": [
    "print(f\"PMC {len(pmc_corps)}\")\n",
    "print(f\"Twitter {len(twitter_corps)}\")\n",
    "\n",
    "print(f\"Train : {len(train)}\")\n",
    "print(f\"Testa : {len(testa)}\")\n",
    "print(f\"Testb : {len(testb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twimed_conll_03/conll_03/eng.testa\n",
      "writing ./twimed_conll_03/conll_03/eng.testa ...\n",
      "1/321\n",
      "101/321\n",
      "201/321\n",
      "301/321\n",
      "./twimed_conll_03/conll_03/eng.testb\n",
      "writing ./twimed_conll_03/conll_03/eng.testb ...\n",
      "1/322\n",
      "101/322\n",
      "201/322\n",
      "301/322\n",
      "./twimed_conll_03/conll_03/eng.train\n",
      "writing ./twimed_conll_03/conll_03/eng.train ...\n",
      "1/964\n",
      "101/964\n",
      "201/964\n",
      "301/964\n",
      "401/964\n",
      "501/964\n",
      "601/964\n",
      "701/964\n",
      "801/964\n",
      "901/964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./twimed_conll_03/conll_03/eng.train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertTwiMedConll_03(testa, \"./twimed_conll_03\", \"eng.testa\")\n",
    "convertTwiMedConll_03(testb, \"./twimed_conll_03\", \"eng.testb\")\n",
    "convertTwiMedConll_03(train, \"./twimed_conll_03\", \"eng.train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 17:56:26,460 Reading data from twimed_conll_03/conll_03\n",
      "2019-03-18 17:56:26,460 Train: twimed_conll_03/conll_03/eng.train\n",
      "2019-03-18 17:56:26,460 Dev: twimed_conll_03/conll_03/eng.testa\n",
      "2019-03-18 17:56:26,461 Test: twimed_conll_03/conll_03/eng.testb\n",
      "[b'<unk>', b'O', b'S-PERSON', b'S-Drug', b'B-CARDINAL', b'I-CARDINAL', b'E-CARDINAL', b'B-Disease_Symptom', b'E-Disease_Symptom', b'I-Disease_Symptom', b'S-ORG', b'S-Disease_Symptom', b'B-DATE', b'I-DATE', b'E-DATE', b'S-CARDINAL', b'B-ORG', b'I-ORG', b'E-ORG', b'S-NORP', b'B-NORP', b'E-NORP', b'S-MONEY', b'S-GPE', b'S-DATE', b'B-TIME', b'E-TIME', b'B-PERSON', b'E-PERSON', b'B-PERCENT', b'E-PERCENT', b'B-MONEY', b'E-MONEY', b'S-ORDINAL', b'B-GPE', b'E-GPE', b'S-PRODUCT', b'I-PERCENT', b'B-Drug', b'E-Drug', b'I-PERSON', b'S-TIME', b'B-QUANTITY', b'E-QUANTITY', b'I-TIME', b'B-LAW', b'E-LAW', b'B-FAC', b'I-FAC', b'E-FAC', b'B-LOC', b'E-LOC', b'S-LOC', b'I-QUANTITY', b'B-PRODUCT', b'E-PRODUCT', b'I-GPE', b'B-WORK_OF_ART', b'I-WORK_OF_ART', b'E-WORK_OF_ART', b'S-LANGUAGE', b'I-LAW', b'I-MONEY', b'B-EVENT', b'I-EVENT', b'E-EVENT', b'<START>', b'<STOP>']\n",
      "2019-03-18 17:56:30,146 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:30,147 Evaluation method: MICRO_F1_SCORE\n",
      "2019-03-18 17:56:30,154 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:31,895 epoch 1 - iter 0/11 - loss 44.17560577\n",
      "2019-03-18 17:56:33,836 epoch 1 - iter 1/11 - loss 40.50722885\n",
      "2019-03-18 17:56:35,826 epoch 1 - iter 2/11 - loss 34.55317688\n",
      "2019-03-18 17:56:38,144 epoch 1 - iter 3/11 - loss 30.59306335\n",
      "2019-03-18 17:56:39,775 epoch 1 - iter 4/11 - loss 27.96236229\n",
      "2019-03-18 17:56:42,063 epoch 1 - iter 5/11 - loss 26.18632952\n",
      "2019-03-18 17:56:43,795 epoch 1 - iter 6/11 - loss 24.40803814\n",
      "2019-03-18 17:56:45,498 epoch 1 - iter 7/11 - loss 23.13035786\n",
      "2019-03-18 17:56:47,117 epoch 1 - iter 8/11 - loss 22.29402754\n",
      "2019-03-18 17:56:49,733 epoch 1 - iter 9/11 - loss 21.61248178\n",
      "2019-03-18 17:56:50,006 epoch 1 - iter 10/11 - loss 21.50063546\n",
      "2019-03-18 17:56:50,020 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:56:50,020 EPOCH 1 done: loss 21.5006 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:56:57,072 DEV  : loss 12.66107559 - f-score 0.0096 - acc 0.0048\n",
      "2019-03-18 17:57:03,573 TEST : loss 12.28554344 - f-score 0.0039 - acc 0.0019\n",
      "2019-03-18 17:57:06,238 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:06,862 epoch 2 - iter 0/11 - loss 14.20663261\n",
      "2019-03-18 17:57:07,569 epoch 2 - iter 1/11 - loss 14.04750347\n",
      "2019-03-18 17:57:08,233 epoch 2 - iter 2/11 - loss 14.07370090\n",
      "2019-03-18 17:57:08,991 epoch 2 - iter 3/11 - loss 13.71464539\n",
      "2019-03-18 17:57:09,797 epoch 2 - iter 4/11 - loss 13.86521835\n",
      "2019-03-18 17:57:10,685 epoch 2 - iter 5/11 - loss 13.76049868\n",
      "2019-03-18 17:57:11,390 epoch 2 - iter 6/11 - loss 13.47089005\n",
      "2019-03-18 17:57:12,091 epoch 2 - iter 7/11 - loss 13.10215271\n",
      "2019-03-18 17:57:13,019 epoch 2 - iter 8/11 - loss 12.92426872\n",
      "2019-03-18 17:57:13,648 epoch 2 - iter 9/11 - loss 12.68688488\n",
      "2019-03-18 17:57:13,874 epoch 2 - iter 10/11 - loss 12.73621159\n",
      "2019-03-18 17:57:13,888 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:13,889 EPOCH 2 done: loss 12.7362 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:16,675 DEV  : loss 9.32568836 - f-score 0.0205 - acc 0.0103\n",
      "2019-03-18 17:57:19,344 TEST : loss 8.97776222 - f-score 0.0128 - acc 0.0064\n",
      "2019-03-18 17:57:24,978 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:25,760 epoch 3 - iter 0/11 - loss 9.99269676\n",
      "2019-03-18 17:57:26,391 epoch 3 - iter 1/11 - loss 10.09763527\n",
      "2019-03-18 17:57:27,091 epoch 3 - iter 2/11 - loss 10.23884328\n",
      "2019-03-18 17:57:28,035 epoch 3 - iter 3/11 - loss 10.53505182\n",
      "2019-03-18 17:57:28,857 epoch 3 - iter 4/11 - loss 10.56975632\n",
      "2019-03-18 17:57:29,564 epoch 3 - iter 5/11 - loss 10.27961842\n",
      "2019-03-18 17:57:30,244 epoch 3 - iter 6/11 - loss 10.25106580\n",
      "2019-03-18 17:57:30,858 epoch 3 - iter 7/11 - loss 10.38041413\n",
      "2019-03-18 17:57:31,566 epoch 3 - iter 8/11 - loss 10.43578243\n",
      "2019-03-18 17:57:32,460 epoch 3 - iter 9/11 - loss 10.41155624\n",
      "2019-03-18 17:57:32,612 epoch 3 - iter 10/11 - loss 10.38365562\n",
      "2019-03-18 17:57:32,626 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:32,627 EPOCH 3 done: loss 10.3837 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:35,415 DEV  : loss 9.12169075 - f-score 0.0214 - acc 0.0108\n",
      "2019-03-18 17:57:38,082 TEST : loss 8.74884129 - f-score 0.0196 - acc 0.0099\n",
      "2019-03-18 17:57:43,157 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:43,907 epoch 4 - iter 0/11 - loss 8.19914436\n",
      "2019-03-18 17:57:44,621 epoch 4 - iter 1/11 - loss 8.74515247\n",
      "2019-03-18 17:57:45,561 epoch 4 - iter 2/11 - loss 9.32699362\n",
      "2019-03-18 17:57:46,455 epoch 4 - iter 3/11 - loss 9.39928699\n",
      "2019-03-18 17:57:47,283 epoch 4 - iter 4/11 - loss 9.75068874\n",
      "2019-03-18 17:57:47,994 epoch 4 - iter 5/11 - loss 9.63561455\n",
      "2019-03-18 17:57:48,588 epoch 4 - iter 6/11 - loss 9.54589203\n",
      "2019-03-18 17:57:49,231 epoch 4 - iter 7/11 - loss 9.49313271\n",
      "2019-03-18 17:57:49,904 epoch 4 - iter 8/11 - loss 9.52759753\n",
      "2019-03-18 17:57:50,606 epoch 4 - iter 9/11 - loss 9.49480448\n",
      "2019-03-18 17:57:50,759 epoch 4 - iter 10/11 - loss 9.46416825\n",
      "2019-03-18 17:57:50,773 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:57:50,774 EPOCH 4 done: loss 9.4642 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:57:53,569 DEV  : loss 7.39441586 - f-score 0.4120 - acc 0.2595\n",
      "2019-03-18 17:57:56,257 TEST : loss 7.06210852 - f-score 0.4233 - acc 0.2684\n",
      "2019-03-18 17:58:01,449 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:02,192 epoch 5 - iter 0/11 - loss 8.43217754\n",
      "2019-03-18 17:58:02,819 epoch 5 - iter 1/11 - loss 8.48410082\n",
      "2019-03-18 17:58:03,523 epoch 5 - iter 2/11 - loss 8.52130985\n",
      "2019-03-18 17:58:04,129 epoch 5 - iter 3/11 - loss 8.41020131\n",
      "2019-03-18 17:58:05,076 epoch 5 - iter 4/11 - loss 8.61309643\n",
      "2019-03-18 17:58:05,755 epoch 5 - iter 5/11 - loss 8.43806251\n",
      "2019-03-18 17:58:06,443 epoch 5 - iter 6/11 - loss 8.52567823\n",
      "2019-03-18 17:58:07,337 epoch 5 - iter 7/11 - loss 8.54056919\n",
      "2019-03-18 17:58:08,152 epoch 5 - iter 8/11 - loss 8.51146412\n",
      "2019-03-18 17:58:08,781 epoch 5 - iter 9/11 - loss 8.47442732\n",
      "2019-03-18 17:58:09,069 epoch 5 - iter 10/11 - loss 8.50992989\n",
      "2019-03-18 17:58:09,083 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:09,083 EPOCH 5 done: loss 8.5099 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:11,879 DEV  : loss 7.08802319 - f-score 0.3310 - acc 0.1983\n",
      "2019-03-18 17:58:14,563 TEST : loss 6.71771431 - f-score 0.3113 - acc 0.1844\n",
      "2019-03-18 17:58:19,736 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:20,426 epoch 6 - iter 0/11 - loss 7.92453337\n",
      "2019-03-18 17:58:21,471 epoch 6 - iter 1/11 - loss 8.77775979\n",
      "2019-03-18 17:58:22,421 epoch 6 - iter 2/11 - loss 8.47536310\n",
      "2019-03-18 17:58:23,036 epoch 6 - iter 3/11 - loss 8.39170980\n",
      "2019-03-18 17:58:23,638 epoch 6 - iter 4/11 - loss 8.32213879\n",
      "2019-03-18 17:58:24,276 epoch 6 - iter 5/11 - loss 8.08560864\n",
      "2019-03-18 17:58:25,090 epoch 6 - iter 6/11 - loss 8.14814935\n",
      "2019-03-18 17:58:25,785 epoch 6 - iter 7/11 - loss 8.01995218\n",
      "2019-03-18 17:58:26,409 epoch 6 - iter 8/11 - loss 7.84352154\n",
      "2019-03-18 17:58:27,107 epoch 6 - iter 9/11 - loss 7.88991308\n",
      "2019-03-18 17:58:27,259 epoch 6 - iter 10/11 - loss 7.89073251\n",
      "2019-03-18 17:58:27,274 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:27,274 EPOCH 6 done: loss 7.8907 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:30,057 DEV  : loss 6.11837435 - f-score 0.4590 - acc 0.2978\n",
      "2019-03-18 17:58:32,721 TEST : loss 5.77262497 - f-score 0.4644 - acc 0.3025\n",
      "2019-03-18 17:58:37,666 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:38,281 epoch 7 - iter 0/11 - loss 6.67020273\n",
      "2019-03-18 17:58:39,101 epoch 7 - iter 1/11 - loss 7.34987855\n",
      "2019-03-18 17:58:39,703 epoch 7 - iter 2/11 - loss 7.15557591\n",
      "2019-03-18 17:58:40,400 epoch 7 - iter 3/11 - loss 6.96355498\n",
      "2019-03-18 17:58:41,010 epoch 7 - iter 4/11 - loss 6.84079437\n",
      "2019-03-18 17:58:41,639 epoch 7 - iter 5/11 - loss 6.92933838\n",
      "2019-03-18 17:58:42,411 epoch 7 - iter 6/11 - loss 7.21978371\n",
      "2019-03-18 17:58:43,097 epoch 7 - iter 7/11 - loss 7.23629302\n",
      "2019-03-18 17:58:43,979 epoch 7 - iter 8/11 - loss 7.21164571\n",
      "2019-03-18 17:58:44,934 epoch 7 - iter 9/11 - loss 7.34780402\n",
      "2019-03-18 17:58:45,119 epoch 7 - iter 10/11 - loss 7.32805607\n",
      "2019-03-18 17:58:45,133 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:45,133 EPOCH 7 done: loss 7.3281 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:58:48,071 DEV  : loss 5.93869591 - f-score 0.3889 - acc 0.2414\n",
      "2019-03-18 17:58:50,739 TEST : loss 5.58505630 - f-score 0.3908 - acc 0.2429\n",
      "2019-03-18 17:58:56,095 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:58:56,798 epoch 8 - iter 0/11 - loss 5.46142101\n",
      "2019-03-18 17:58:57,439 epoch 8 - iter 1/11 - loss 5.89010191\n",
      "2019-03-18 17:58:58,174 epoch 8 - iter 2/11 - loss 6.34718418\n",
      "2019-03-18 17:58:58,924 epoch 8 - iter 3/11 - loss 6.30638206\n",
      "2019-03-18 17:58:59,733 epoch 8 - iter 4/11 - loss 6.28721876\n",
      "2019-03-18 17:59:00,435 epoch 8 - iter 5/11 - loss 6.44984404\n",
      "2019-03-18 17:59:01,053 epoch 8 - iter 6/11 - loss 6.61887836\n",
      "2019-03-18 17:59:01,721 epoch 8 - iter 7/11 - loss 6.41014194\n",
      "2019-03-18 17:59:02,678 epoch 8 - iter 8/11 - loss 6.64705234\n",
      "2019-03-18 17:59:03,588 epoch 8 - iter 9/11 - loss 6.74860792\n",
      "2019-03-18 17:59:03,799 epoch 8 - iter 10/11 - loss 6.74168421\n",
      "2019-03-18 17:59:03,814 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:03,814 EPOCH 8 done: loss 6.7417 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:06,638 DEV  : loss 5.36948490 - f-score 0.4989 - acc 0.3324\n",
      "2019-03-18 17:59:09,328 TEST : loss 5.03092718 - f-score 0.5176 - acc 0.3492\n",
      "2019-03-18 17:59:14,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:15,344 epoch 9 - iter 0/11 - loss 7.17939377\n",
      "2019-03-18 17:59:16,056 epoch 9 - iter 1/11 - loss 7.15201283\n",
      "2019-03-18 17:59:16,761 epoch 9 - iter 2/11 - loss 7.26948770\n",
      "2019-03-18 17:59:17,697 epoch 9 - iter 3/11 - loss 7.16156352\n",
      "2019-03-18 17:59:18,397 epoch 9 - iter 4/11 - loss 6.71659307\n",
      "2019-03-18 17:59:19,039 epoch 9 - iter 5/11 - loss 6.43846965\n",
      "2019-03-18 17:59:19,940 epoch 9 - iter 6/11 - loss 6.41128308\n",
      "2019-03-18 17:59:20,546 epoch 9 - iter 7/11 - loss 6.21924782\n",
      "2019-03-18 17:59:21,203 epoch 9 - iter 8/11 - loss 6.23941856\n",
      "2019-03-18 17:59:21,969 epoch 9 - iter 9/11 - loss 6.31300049\n",
      "2019-03-18 17:59:22,130 epoch 9 - iter 10/11 - loss 6.27388543\n",
      "2019-03-18 17:59:22,143 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:22,144 EPOCH 9 done: loss 6.2739 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:24,924 DEV  : loss 5.07834387 - f-score 0.4987 - acc 0.3321\n",
      "2019-03-18 17:59:27,595 TEST : loss 4.70623636 - f-score 0.5009 - acc 0.3342\n",
      "2019-03-18 17:59:32,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:33,737 epoch 10 - iter 0/11 - loss 6.25418997\n",
      "2019-03-18 17:59:34,438 epoch 10 - iter 1/11 - loss 5.71040273\n",
      "2019-03-18 17:59:35,148 epoch 10 - iter 2/11 - loss 5.84132926\n",
      "2019-03-18 17:59:36,083 epoch 10 - iter 3/11 - loss 5.89833522\n",
      "2019-03-18 17:59:36,723 epoch 10 - iter 4/11 - loss 5.79887238\n",
      "2019-03-18 17:59:37,403 epoch 10 - iter 5/11 - loss 5.74698464\n",
      "2019-03-18 17:59:38,115 epoch 10 - iter 6/11 - loss 5.78182772\n",
      "2019-03-18 17:59:39,033 epoch 10 - iter 7/11 - loss 5.96230614\n",
      "2019-03-18 17:59:39,734 epoch 10 - iter 8/11 - loss 5.90691784\n",
      "2019-03-18 17:59:40,370 epoch 10 - iter 9/11 - loss 5.84485650\n",
      "2019-03-18 17:59:40,615 epoch 10 - iter 10/11 - loss 5.85160390\n",
      "2019-03-18 17:59:40,629 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:40,629 EPOCH 10 done: loss 5.8516 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 17:59:43,441 DEV  : loss 4.95747089 - f-score 0.4591 - acc 0.2980\n",
      "2019-03-18 17:59:46,129 TEST : loss 4.59088039 - f-score 0.4632 - acc 0.3014\n",
      "2019-03-18 17:59:51,124 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:51,654 epoch 11 - iter 0/11 - loss 5.71006632\n",
      "2019-03-18 17:59:52,332 epoch 11 - iter 1/11 - loss 5.64222813\n",
      "2019-03-18 17:59:53,221 epoch 11 - iter 2/11 - loss 5.55693849\n",
      "2019-03-18 17:59:53,976 epoch 11 - iter 3/11 - loss 5.84419525\n",
      "2019-03-18 17:59:54,684 epoch 11 - iter 4/11 - loss 5.79595842\n",
      "2019-03-18 17:59:55,295 epoch 11 - iter 5/11 - loss 5.84953348\n",
      "2019-03-18 17:59:55,904 epoch 11 - iter 6/11 - loss 5.78511463\n",
      "2019-03-18 17:59:56,722 epoch 11 - iter 7/11 - loss 5.74038023\n",
      "2019-03-18 17:59:57,354 epoch 11 - iter 8/11 - loss 5.67391464\n",
      "2019-03-18 17:59:57,960 epoch 11 - iter 9/11 - loss 5.67721190\n",
      "2019-03-18 17:59:58,401 epoch 11 - iter 10/11 - loss 5.69079367\n",
      "2019-03-18 17:59:58,414 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 17:59:58,415 EPOCH 11 done: loss 5.6908 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:01,228 DEV  : loss 4.66452551 - f-score 0.5329 - acc 0.3632\n",
      "2019-03-18 18:00:03,916 TEST : loss 4.25875139 - f-score 0.5418 - acc 0.3716\n",
      "2019-03-18 18:00:09,743 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:10,468 epoch 12 - iter 0/11 - loss 5.96336031\n",
      "2019-03-18 18:00:11,113 epoch 12 - iter 1/11 - loss 5.62485313\n",
      "2019-03-18 18:00:11,707 epoch 12 - iter 2/11 - loss 5.42428382\n",
      "2019-03-18 18:00:12,408 epoch 12 - iter 3/11 - loss 5.54216826\n",
      "2019-03-18 18:00:13,034 epoch 12 - iter 4/11 - loss 5.41737337\n",
      "2019-03-18 18:00:13,700 epoch 12 - iter 5/11 - loss 5.32737501\n",
      "2019-03-18 18:00:14,599 epoch 12 - iter 6/11 - loss 5.41365726\n",
      "2019-03-18 18:00:15,534 epoch 12 - iter 7/11 - loss 5.29460144\n",
      "2019-03-18 18:00:16,354 epoch 12 - iter 8/11 - loss 5.25848685\n",
      "2019-03-18 18:00:17,073 epoch 12 - iter 9/11 - loss 5.39782934\n",
      "2019-03-18 18:00:17,241 epoch 12 - iter 10/11 - loss 5.39045261\n",
      "2019-03-18 18:00:17,254 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:17,255 EPOCH 12 done: loss 5.3905 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:20,065 DEV  : loss 4.43521547 - f-score 0.5669 - acc 0.3955\n",
      "2019-03-18 18:00:22,754 TEST : loss 4.09914017 - f-score 0.5771 - acc 0.4057\n",
      "2019-03-18 18:00:28,016 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:28,649 epoch 13 - iter 0/11 - loss 4.60199356\n",
      "2019-03-18 18:00:29,596 epoch 13 - iter 1/11 - loss 5.16084981\n",
      "2019-03-18 18:00:30,399 epoch 13 - iter 2/11 - loss 5.16295862\n",
      "2019-03-18 18:00:31,033 epoch 13 - iter 3/11 - loss 5.25921547\n",
      "2019-03-18 18:00:31,787 epoch 13 - iter 4/11 - loss 5.21205482\n",
      "2019-03-18 18:00:32,490 epoch 13 - iter 5/11 - loss 5.19422070\n",
      "2019-03-18 18:00:33,142 epoch 13 - iter 6/11 - loss 5.34494202\n",
      "2019-03-18 18:00:34,038 epoch 13 - iter 7/11 - loss 5.21362990\n",
      "2019-03-18 18:00:34,748 epoch 13 - iter 8/11 - loss 5.26232640\n",
      "2019-03-18 18:00:35,360 epoch 13 - iter 9/11 - loss 5.22053423\n",
      "2019-03-18 18:00:35,653 epoch 13 - iter 10/11 - loss 5.21047910\n",
      "2019-03-18 18:00:35,667 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:35,667 EPOCH 13 done: loss 5.2105 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:38,485 DEV  : loss 4.34682655 - f-score 0.5686 - acc 0.3972\n",
      "2019-03-18 18:00:41,187 TEST : loss 4.02324915 - f-score 0.5768 - acc 0.4053\n",
      "2019-03-18 18:00:46,419 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:47,079 epoch 14 - iter 0/11 - loss 4.25793743\n",
      "2019-03-18 18:00:47,799 epoch 14 - iter 1/11 - loss 4.46407008\n",
      "2019-03-18 18:00:48,435 epoch 14 - iter 2/11 - loss 4.47958120\n",
      "2019-03-18 18:00:49,045 epoch 14 - iter 3/11 - loss 4.58879018\n",
      "2019-03-18 18:00:49,997 epoch 14 - iter 4/11 - loss 4.89532642\n",
      "2019-03-18 18:00:50,715 epoch 14 - iter 5/11 - loss 4.92798138\n",
      "2019-03-18 18:00:51,359 epoch 14 - iter 6/11 - loss 4.87303584\n",
      "2019-03-18 18:00:52,121 epoch 14 - iter 7/11 - loss 4.85738635\n",
      "2019-03-18 18:00:52,800 epoch 14 - iter 8/11 - loss 5.00722196\n",
      "2019-03-18 18:00:53,604 epoch 14 - iter 9/11 - loss 4.87391279\n",
      "2019-03-18 18:00:53,785 epoch 14 - iter 10/11 - loss 4.90385286\n",
      "2019-03-18 18:00:53,799 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:00:53,799 EPOCH 14 done: loss 4.9039 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:00:56,608 DEV  : loss 4.20933580 - f-score 0.5594 - acc 0.3883\n",
      "2019-03-18 18:00:59,297 TEST : loss 3.84771705 - f-score 0.5717 - acc 0.4003\n",
      "2019-03-18 18:01:04,307 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:05,013 epoch 15 - iter 0/11 - loss 4.30351925\n",
      "2019-03-18 18:01:05,666 epoch 15 - iter 1/11 - loss 4.59920287\n",
      "2019-03-18 18:01:06,197 epoch 15 - iter 2/11 - loss 4.40035756\n",
      "2019-03-18 18:01:06,870 epoch 15 - iter 3/11 - loss 4.69872618\n",
      "2019-03-18 18:01:07,577 epoch 15 - iter 4/11 - loss 4.61874752\n",
      "2019-03-18 18:01:08,514 epoch 15 - iter 5/11 - loss 4.49880425\n",
      "2019-03-18 18:01:09,334 epoch 15 - iter 6/11 - loss 4.59480470\n",
      "2019-03-18 18:01:10,041 epoch 15 - iter 7/11 - loss 4.65069973\n",
      "2019-03-18 18:01:10,639 epoch 15 - iter 8/11 - loss 4.67297787\n",
      "2019-03-18 18:01:11,538 epoch 15 - iter 9/11 - loss 4.68370848\n",
      "2019-03-18 18:01:11,736 epoch 15 - iter 10/11 - loss 4.70204913\n",
      "2019-03-18 18:01:11,750 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:11,750 EPOCH 15 done: loss 4.7020 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:14,561 DEV  : loss 4.16310740 - f-score 0.5905 - acc 0.4190\n",
      "2019-03-18 18:01:17,255 TEST : loss 3.84660769 - f-score 0.6050 - acc 0.4337\n",
      "2019-03-18 18:01:22,230 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:22,862 epoch 16 - iter 0/11 - loss 3.54200029\n",
      "2019-03-18 18:01:23,628 epoch 16 - iter 1/11 - loss 4.15624690\n",
      "2019-03-18 18:01:24,324 epoch 16 - iter 2/11 - loss 4.29155906\n",
      "2019-03-18 18:01:25,278 epoch 16 - iter 3/11 - loss 4.83133113\n",
      "2019-03-18 18:01:25,958 epoch 16 - iter 4/11 - loss 4.83491411\n",
      "2019-03-18 18:01:26,583 epoch 16 - iter 5/11 - loss 4.76907198\n",
      "2019-03-18 18:01:27,482 epoch 16 - iter 6/11 - loss 4.87205798\n",
      "2019-03-18 18:01:28,197 epoch 16 - iter 7/11 - loss 4.81924462\n",
      "2019-03-18 18:01:28,752 epoch 16 - iter 8/11 - loss 4.69889336\n",
      "2019-03-18 18:01:29,454 epoch 16 - iter 9/11 - loss 4.63282320\n",
      "2019-03-18 18:01:29,643 epoch 16 - iter 10/11 - loss 4.62375697\n",
      "2019-03-18 18:01:29,657 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:29,657 EPOCH 16 done: loss 4.6238 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:32,448 DEV  : loss 3.99411488 - f-score 0.5680 - acc 0.3966\n",
      "2019-03-18 18:01:35,124 TEST : loss 3.64926696 - f-score 0.5800 - acc 0.4085\n",
      "2019-03-18 18:01:40,335 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:40,992 epoch 17 - iter 0/11 - loss 4.51284742\n",
      "2019-03-18 18:01:41,805 epoch 17 - iter 1/11 - loss 4.77409673\n",
      "2019-03-18 18:01:42,486 epoch 17 - iter 2/11 - loss 4.55030553\n",
      "2019-03-18 18:01:43,135 epoch 17 - iter 3/11 - loss 4.58373928\n",
      "2019-03-18 18:01:43,891 epoch 17 - iter 4/11 - loss 4.58858814\n",
      "2019-03-18 18:01:44,523 epoch 17 - iter 5/11 - loss 4.63158258\n",
      "2019-03-18 18:01:45,456 epoch 17 - iter 6/11 - loss 4.43642787\n",
      "2019-03-18 18:01:46,168 epoch 17 - iter 7/11 - loss 4.51382011\n",
      "2019-03-18 18:01:47,062 epoch 17 - iter 8/11 - loss 4.54163424\n",
      "2019-03-18 18:01:47,781 epoch 17 - iter 9/11 - loss 4.54151521\n",
      "2019-03-18 18:01:47,937 epoch 17 - iter 10/11 - loss 4.53877623\n",
      "2019-03-18 18:01:47,953 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:47,953 EPOCH 17 done: loss 4.5388 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:01:50,766 DEV  : loss 3.85810399 - f-score 0.5922 - acc 0.4206\n",
      "2019-03-18 18:01:53,455 TEST : loss 3.54655409 - f-score 0.6145 - acc 0.4435\n",
      "2019-03-18 18:01:58,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:01:59,345 epoch 18 - iter 0/11 - loss 4.32796431\n",
      "2019-03-18 18:01:59,955 epoch 18 - iter 1/11 - loss 4.73040605\n",
      "2019-03-18 18:02:00,718 epoch 18 - iter 2/11 - loss 4.47780776\n",
      "2019-03-18 18:02:01,431 epoch 18 - iter 3/11 - loss 4.54662472\n",
      "2019-03-18 18:02:02,388 epoch 18 - iter 4/11 - loss 4.58515563\n",
      "2019-03-18 18:02:03,072 epoch 18 - iter 5/11 - loss 4.50415059\n",
      "2019-03-18 18:02:03,788 epoch 18 - iter 6/11 - loss 4.40703028\n",
      "2019-03-18 18:02:04,681 epoch 18 - iter 7/11 - loss 4.45792547\n",
      "2019-03-18 18:02:05,490 epoch 18 - iter 8/11 - loss 4.39516436\n",
      "2019-03-18 18:02:06,139 epoch 18 - iter 9/11 - loss 4.31525629\n",
      "2019-03-18 18:02:06,336 epoch 18 - iter 10/11 - loss 4.31545827\n",
      "2019-03-18 18:02:06,349 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 18:02:06,350 EPOCH 18 done: loss 4.3155 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 18:02:09,167 DEV  : loss 3.82240725 - f-score 0.6072 - acc 0.4360\n"
     ]
    }
   ],
   "source": [
    "from jsonCorps2conll03 import rmdir\n",
    "from flairNER import train\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "conll_03_corps_folder = 'twimed_conll_03'\n",
    "model_output_folder = 'twimed-ner'\n",
    "rmdir(conll_03_corps_folder)\n",
    "train(conll_03_corps_folder, model_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 14:13:20,987 loading file twimed-ner/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "model_output_folder = 'twimed-ner'\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence(\"\"\"\n",
    "Previous studies have demonstrated that glucocorticoid hormones, including dexamethasone, induced alterations in intracellular calcium homeostasis in acute lymphoblastic leukemia (ALL) cells. However, the mechanism by which intracellular calcium homeostasis participates in dexamethasone sensitivity and resistance on ALL cells remains elusive. Here, we found that treatment of cells with dexamethasone resulted in increased intracellular calcium concentrations through store-operated calcium entry stimulation, which was curtailed by store-operated calcium channel blockers. We show that BAPTA-AM, an intracellular Ca2+ chelator, synergistically enhances dexamethasone lethality in two human ALL cell lines and in three primary specimens. This effect correlated with the inhibition of the prosurvival kinase ERK1/2 signaling pathway. Chelating intracellular calcium with Bapta-AM or inhibiting ERK1/2 with PD98059 significantly potentiated dexamethasone-induced mitochondrial membrane potential collapse, reactive oxygen species production, cytochrome c release, caspase-3 activity, and cell death. Moreover, we show that thapsigargin elevates intracellular free calcium ion level, and activates ERK1/2 signaling, resulting in the inhibition of dexamethasone-induced ALL cells apoptosis. Together, these results indicate that calcium-related ERK1/2 signaling pathway contributes to protect cells from dexamethasone sensitivity by limiting mitochondrial apoptotic pathway. This report provides a novel resistance pathway underlying the regulatory effect of dexamethasone on ALL cells.\n",
    "\"\"\")\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load_from_file(model_output_folder + '/final-model.pt')\n",
    "\n",
    "def detect(tagger, text):\n",
    "    \n",
    "    print('===============================================')\n",
    "    text = text.replace('#', '')\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    print(sentence)\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # iterate over entities and print\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(entity)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"Starting back on fluoxetine tonight, I did pick up the prescription a few days ago but in the past when I’ve started/upped meds I’ve had to call in sick to work due to side effects. So I waited. Now I have 3 days off to adjust\" - 46 Tokens\n",
      "--------------------------------\n",
      "Drug-span [4]: \"fluoxetine\"\n",
      "DATE-span [13,14]: \"few days\"\n",
      "DATE-span [42,43]: \"3 days\"\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "detect(tagger, \"\"\"Starting back on fluoxetine tonight, I did pick up the prescription a few days ago but in the past when I’ve started/upped meds I’ve had to call in sick to work due to side effects. So I waited. Now I have 3 days off to adjust\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"Publication alert: combining CBT with fluoxetine might be superior to either therapy for adolescents with depression. Model-based random forest method applied in a study by @HeidiBaya Seibold, T.Hothorn, S.Foster, M.Mohler-Kuo,\" - 30 Tokens\n",
      "--------------------------------\n",
      "ORG-span [4]: \"CBT\"\n",
      "Drug-span [6]: \"fluoxetine\"\n",
      "ORG-span [26,27]: \"@HeidiBaya Seibold,\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"Publication alert: combining CBT with #fluoxetine might be superior to either therapy for adolescents with #depression. Model-based random forest method applied in a study by @HeidiBaya Seibold, T.Hothorn, S.Foster, M.Mohler-Kuo, \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"sleepless nights, feeling worthless, lifes trash, this shit aint worth it man, fluoxetine isnt doing nothing. ive come to conclusion life is fucking worthless and i wish everyone the best, fuck this, i cant handle this shit mentally anymore, fuck life, im done with life. bye\" - 46 Tokens\n",
      "--------------------------------\n",
      "Drug-span [13]: \"fluoxetine\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"sleepless nights, feeling worthless, lifes trash, this shit aint worth it man, fluoxetine isnt doing nothing. ive come to conclusion life is fucking worthless and i wish everyone the best, fuck this, i cant handle this shit mentally anymore, fuck life,  im done with life.  bye\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Sentence: \"i’m i’m terrified of putting anything in my body (that’s why i don’t smoke and rarely drink). i’m even scared of taking tylenol sometimes. antibiotics sound like the end of the world to me. idk. help.\" - 36 Tokens\n",
      "--------------------------------\n",
      "Disease_Symptom-span [14]: \"smoke\"\n",
      "Drug-span [23]: \"tylenol\"\n"
     ]
    }
   ],
   "source": [
    "detect(tagger, \"\"\"i’m i’m terrified of putting anything in my body (that’s why i don’t smoke and rarely drink). i’m even scared of taking tylenol sometimes. antibiotics sound like the end of the world to me. idk. help.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Briviact since:2006-03-21 until:2006-11-13', 'Briviact since:2006-11-13 until:2007-07-08', 'Briviact since:2007-07-08 until:2008-03-01', 'Briviact since:2008-03-01 until:2008-10-25', 'Briviact since:2008-10-25 until:2009-06-19', 'Briviact since:2009-06-19 until:2010-02-11', 'Briviact since:2010-02-11 until:2010-10-07', 'Briviact since:2010-10-07 until:2011-06-01', 'Briviact since:2011-06-01 until:2012-01-24', 'Briviact since:2012-01-24 until:2012-09-18', 'Briviact since:2012-09-18 until:2013-05-13', 'Briviact since:2013-05-13 until:2014-01-05', 'Briviact since:2014-01-05 until:2014-08-30', 'Briviact since:2014-08-30 until:2015-04-25', 'Briviact since:2015-04-25 until:2015-12-18', 'Briviact since:2015-12-18 until:2016-08-11', 'Briviact since:2016-08-11 until:2017-04-06', 'Briviact since:2017-04-06 until:2017-11-29', 'Briviact since:2017-11-29 until:2018-07-24', 'Briviact since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,815 queries: ['Briviact since:2006-03-21 until:2006-11-13', 'Briviact since:2006-11-13 until:2007-07-08', 'Briviact since:2007-07-08 until:2008-03-01', 'Briviact since:2008-03-01 until:2008-10-25', 'Briviact since:2008-10-25 until:2009-06-19', 'Briviact since:2009-06-19 until:2010-02-11', 'Briviact since:2010-02-11 until:2010-10-07', 'Briviact since:2010-10-07 until:2011-06-01', 'Briviact since:2011-06-01 until:2012-01-24', 'Briviact since:2012-01-24 until:2012-09-18', 'Briviact since:2012-09-18 until:2013-05-13', 'Briviact since:2013-05-13 until:2014-01-05', 'Briviact since:2014-01-05 until:2014-08-30', 'Briviact since:2014-08-30 until:2015-04-25', 'Briviact since:2015-04-25 until:2015-12-18', 'Briviact since:2015-12-18 until:2016-08-11', 'Briviact since:2016-08-11 until:2017-04-06', 'Briviact since:2017-04-06 until:2017-11-29', 'Briviact since:2017-11-29 until:2018-07-24', 'Briviact since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2009-06-19 until:2010-02-11\n",
      "INFO: Querying Briviact since:2007-07-08 until:2008-03-01\n",
      "INFO: Querying Briviact since:2008-10-25 until:2009-06-19\n",
      "INFO: Querying Briviact since:2006-03-21 until:2006-11-13\n",
      "INFO: Querying Briviact since:2010-10-07 until:2011-06-01\n",
      "INFO: Querying Briviact since:2010-02-11 until:2010-10-07\n",
      "INFO: Querying Briviact since:2008-03-01 until:2008-10-25\n",
      "INFO: Querying Briviact since:2006-11-13 until:2007-07-08\n",
      "INFO: Querying Briviact since:2012-01-24 until:2012-09-18\n",
      "INFO: Querying Briviact since:2014-01-05 until:2014-08-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,817 Querying Briviact since:2006-03-21 until:2006-11-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2012-09-18 until:2013-05-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,819 Querying Briviact since:2012-01-24 until:2012-09-18\n",
      "2019-03-19 16:28:59,817 Querying Briviact since:2008-10-25 until:2009-06-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2013-05-13 until:2014-01-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,819 Querying Briviact since:2010-10-07 until:2011-06-01\n",
      "2019-03-19 16:28:59,817 Querying Briviact since:2007-07-08 until:2008-03-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2011-06-01 until:2012-01-24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,819 Querying Briviact since:2010-02-11 until:2010-10-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2014-08-30 until:2015-04-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,817 Querying Briviact since:2008-03-01 until:2008-10-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2016-08-11 until:2017-04-06\n",
      "INFO: Querying Briviact since:2015-04-25 until:2015-12-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,819 Querying Briviact since:2011-06-01 until:2012-01-24\n",
      "2019-03-19 16:28:59,818 Querying Briviact since:2009-06-19 until:2010-02-11\n",
      "2019-03-19 16:28:59,820 Querying Briviact since:2014-01-05 until:2014-08-30\n",
      "2019-03-19 16:28:59,819 Querying Briviact since:2012-09-18 until:2013-05-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2017-11-29 until:2018-07-24\n",
      "INFO: Querying Briviact since:2017-04-06 until:2017-11-29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,819 Querying Briviact since:2013-05-13 until:2014-01-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Briviact since:2018-07-24 until:2019-03-19\n",
      "INFO: Querying Briviact since:2015-12-18 until:2016-08-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:28:59,821 Querying Briviact since:2014-08-30 until:2015-04-25\n",
      "2019-03-19 16:28:59,823 Querying Briviact since:2017-11-29 until:2018-07-24\n",
      "2019-03-19 16:28:59,817 Querying Briviact since:2006-11-13 until:2007-07-08\n",
      "2019-03-19 16:28:59,822 Querying Briviact since:2015-12-18 until:2016-08-11\n",
      "2019-03-19 16:28:59,822 Querying Briviact since:2017-04-06 until:2017-11-29\n",
      "2019-03-19 16:28:59,822 Querying Briviact since:2016-08-11 until:2017-04-06\n",
      "2019-03-19 16:28:59,823 Querying Briviact since:2018-07-24 until:2019-03-19\n",
      "2019-03-19 16:28:59,822 Querying Briviact since:2015-04-25 until:2015-12-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Briviact%20since%3A2012-09-18%20until%3A2013-05-13.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2010-10-07%20until%3A2011-06-01.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,872 Got 0 tweets for Briviact%20since%3A2012-09-18%20until%3A2013-05-13.\n",
      "2019-03-19 16:29:00,891 Got 0 tweets for Briviact%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,907 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Briviact%20since%3A2008-10-25%20until%3A2009-06-19.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2011-06-01%20until%3A2012-01-24.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2006-11-13%20until%3A2007-07-08.\n",
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,918 Got 0 tweets (0 new).\n",
      "2019-03-19 16:29:00,876 Got 0 tweets for Briviact%20since%3A2010-10-07%20until%3A2011-06-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,924 Got 0 tweets (0 new).\n",
      "2019-03-19 16:29:00,899 Got 0 tweets for Briviact%20since%3A2006-11-13%20until%3A2007-07-08.\n",
      "2019-03-19 16:29:00,896 Got 0 tweets for Briviact%20since%3A2011-06-01%20until%3A2012-01-24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,931 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,932 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Briviact%20since%3A2006-03-21%20until%3A2006-11-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,901 Got 0 tweets for Briviact%20since%3A2008-10-25%20until%3A2009-06-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,939 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Briviact%20since%3A2013-05-13%20until%3A2014-01-05.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2010-02-11%20until%3A2010-10-07.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2012-01-24%20until%3A2012-09-18.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2014-01-05%20until%3A2014-08-30.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2008-03-01%20until%3A2008-10-25.\n",
      "INFO: Got 0 tweets for Briviact%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,949 Got 0 tweets for Briviact%20since%3A2010-02-11%20until%3A2010-10-07.\n",
      "2019-03-19 16:29:00,928 Got 0 tweets for Briviact%20since%3A2014-01-05%20until%3A2014-08-30.\n",
      "2019-03-19 16:29:00,950 Got 0 tweets for Briviact%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Briviact%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,946 Got 0 tweets for Briviact%20since%3A2008-03-01%20until%3A2008-10-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,942 Got 0 tweets for Briviact%20since%3A2013-05-13%20until%3A2014-01-05.\n",
      "2019-03-19 16:29:00,925 Got 0 tweets for Briviact%20since%3A2006-03-21%20until%3A2006-11-13.\n",
      "2019-03-19 16:29:00,961 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,963 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,964 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,949 Got 0 tweets for Briviact%20since%3A2012-01-24%20until%3A2012-09-18.\n",
      "2019-03-19 16:29:00,970 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,971 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,972 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,973 Got 0 tweets (0 new).\n",
      "2019-03-19 16:29:00,958 Got 0 tweets for Briviact%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,977 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Briviact%20since%3A2016-08-11%20until%3A2017-04-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,987 Got 20 tweets for Briviact%20since%3A2016-08-11%20until%3A2017-04-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:00,989 Got 20 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Briviact%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,017 Got 20 tweets for Briviact%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,019 Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Briviact%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,040 Got 20 tweets for Briviact%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 60 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,041 Got 60 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Briviact%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,056 Got 20 tweets for Briviact%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 80 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,058 Got 80 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 18 tweets for Briviact%20since%3A2015-04-25%20until%3A2015-12-18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,060 Got 18 tweets for Briviact%20since%3A2015-04-25%20until%3A2015-12-18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 98 tweets (18 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,062 Got 98 tweets (18 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Briviact%20since%3A2017-11-29%20until%3A2018-07-24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,073 Got 20 tweets for Briviact%20since%3A2017-11-29%20until%3A2018-07-24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 118 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:29:01,075 Got 118 tweets (20 new).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "class TwitterWatch: \n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "        \n",
    "    def query(self, product):\n",
    "        list_of_tweets = query_tweets(product, 1)\n",
    "        for tweet in list_of_tweets:\n",
    "            source_entity = self.getSourceEntity(tweet.fullname)\n",
    "            tweet.text_entities = self.getEntityTags(tweet.text)\n",
    "            tweet.source_entity = source_entity\n",
    "            tweet.hasDrug = 'Drug' in tweet.text_entities\n",
    "            tweet.hasSymptom = 'Disease_Symptom' in tweet.text_entities\n",
    "        return list_of_tweets\n",
    "        \n",
    "    def getSourceEntity(self, name):\n",
    "        result = 'UNKNOWN'\n",
    "        entities = self.getEntities(name)\n",
    "        for entity in entities:\n",
    "            result = entity.tag\n",
    "        return result\n",
    "    \n",
    "    def getEntityTags(self, text):\n",
    "        return list(map( lambda entity: entity.tag, self.getEntities(text) ))\n",
    "        \n",
    "        \n",
    "    def getEntities(self, text):\n",
    "        text = self.cleanTwitterTag(text)\n",
    "        if(len(text)>10):\n",
    "            sentence = Sentence(text)\n",
    "            self.tagger.predict(sentence)\n",
    "            return sentence.get_spans('ner')\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def cleanTwitterTag(self, text):\n",
    "        text = text.replace('#', '').replace(\"@\", '')\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['fluoxetine since:2006-03-21 until:2006-11-13', 'fluoxetine since:2006-11-13 until:2007-07-08', 'fluoxetine since:2007-07-08 until:2008-03-01', 'fluoxetine since:2008-03-01 until:2008-10-25', 'fluoxetine since:2008-10-25 until:2009-06-19', 'fluoxetine since:2009-06-19 until:2010-02-11', 'fluoxetine since:2010-02-11 until:2010-10-07', 'fluoxetine since:2010-10-07 until:2011-06-01', 'fluoxetine since:2011-06-01 until:2012-01-24', 'fluoxetine since:2012-01-24 until:2012-09-18', 'fluoxetine since:2012-09-18 until:2013-05-13', 'fluoxetine since:2013-05-13 until:2014-01-05', 'fluoxetine since:2014-01-05 until:2014-08-30', 'fluoxetine since:2014-08-30 until:2015-04-25', 'fluoxetine since:2015-04-25 until:2015-12-18', 'fluoxetine since:2015-12-18 until:2016-08-11', 'fluoxetine since:2016-08-11 until:2017-04-06', 'fluoxetine since:2017-04-06 until:2017-11-29', 'fluoxetine since:2017-11-29 until:2018-07-24', 'fluoxetine since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,921 queries: ['fluoxetine since:2006-03-21 until:2006-11-13', 'fluoxetine since:2006-11-13 until:2007-07-08', 'fluoxetine since:2007-07-08 until:2008-03-01', 'fluoxetine since:2008-03-01 until:2008-10-25', 'fluoxetine since:2008-10-25 until:2009-06-19', 'fluoxetine since:2009-06-19 until:2010-02-11', 'fluoxetine since:2010-02-11 until:2010-10-07', 'fluoxetine since:2010-10-07 until:2011-06-01', 'fluoxetine since:2011-06-01 until:2012-01-24', 'fluoxetine since:2012-01-24 until:2012-09-18', 'fluoxetine since:2012-09-18 until:2013-05-13', 'fluoxetine since:2013-05-13 until:2014-01-05', 'fluoxetine since:2014-01-05 until:2014-08-30', 'fluoxetine since:2014-08-30 until:2015-04-25', 'fluoxetine since:2015-04-25 until:2015-12-18', 'fluoxetine since:2015-12-18 until:2016-08-11', 'fluoxetine since:2016-08-11 until:2017-04-06', 'fluoxetine since:2017-04-06 until:2017-11-29', 'fluoxetine since:2017-11-29 until:2018-07-24', 'fluoxetine since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2007-07-08 until:2008-03-01\n",
      "INFO: Querying fluoxetine since:2008-10-25 until:2009-06-19\n",
      "INFO: Querying fluoxetine since:2006-03-21 until:2006-11-13\n",
      "INFO: Querying fluoxetine since:2008-03-01 until:2008-10-25\n",
      "INFO: Querying fluoxetine since:2009-06-19 until:2010-02-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2007-07-08 until:2008-03-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2006-11-13 until:2007-07-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2008-10-25 until:2009-06-19\n",
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2008-03-01 until:2008-10-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2010-02-11 until:2010-10-07\n",
      "INFO: Querying fluoxetine since:2011-06-01 until:2012-01-24\n",
      "INFO: Querying fluoxetine since:2012-09-18 until:2013-05-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2006-03-21 until:2006-11-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2012-01-24 until:2012-09-18\n",
      "INFO: Querying fluoxetine since:2014-01-05 until:2014-08-30\n",
      "INFO: Querying fluoxetine since:2015-04-25 until:2015-12-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,926 Querying fluoxetine since:2014-01-05 until:2014-08-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2016-08-11 until:2017-04-06\n",
      "INFO: Querying fluoxetine since:2014-08-30 until:2015-04-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,926 Querying fluoxetine since:2012-09-18 until:2013-05-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2013-05-13 until:2014-01-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2006-11-13 until:2007-07-08\n",
      "2019-03-19 18:20:18,925 Querying fluoxetine since:2011-06-01 until:2012-01-24\n",
      "2019-03-19 18:20:18,928 Querying fluoxetine since:2016-08-11 until:2017-04-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2010-10-07 until:2011-06-01\n",
      "INFO: Querying fluoxetine since:2015-12-18 until:2016-08-11\n",
      "INFO: Querying fluoxetine since:2018-07-24 until:2019-03-19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,927 Querying fluoxetine since:2015-04-25 until:2015-12-18\n",
      "2019-03-19 18:20:18,923 Querying fluoxetine since:2009-06-19 until:2010-02-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2017-11-29 until:2018-07-24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,927 Querying fluoxetine since:2014-08-30 until:2015-04-25\n",
      "2019-03-19 18:20:18,926 Querying fluoxetine since:2012-01-24 until:2012-09-18\n",
      "2019-03-19 18:20:18,925 Querying fluoxetine since:2010-02-11 until:2010-10-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying fluoxetine since:2017-04-06 until:2017-11-29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:18,932 Querying fluoxetine since:2018-07-24 until:2019-03-19\n",
      "2019-03-19 18:20:18,925 Querying fluoxetine since:2010-10-07 until:2011-06-01\n",
      "2019-03-19 18:20:18,928 Querying fluoxetine since:2015-12-18 until:2016-08-11\n",
      "2019-03-19 18:20:18,926 Querying fluoxetine since:2013-05-13 until:2014-01-05\n",
      "2019-03-19 18:20:18,930 Querying fluoxetine since:2017-04-06 until:2017-11-29\n",
      "2019-03-19 18:20:18,931 Querying fluoxetine since:2017-11-29 until:2018-07-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for fluoxetine%20since%3A2006-03-21%20until%3A2006-11-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,317 Got 0 tweets for fluoxetine%20since%3A2006-03-21%20until%3A2006-11-13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,319 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 1 tweets for fluoxetine%20since%3A2006-11-13%20until%3A2007-07-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,336 Got 1 tweets for fluoxetine%20since%3A2006-11-13%20until%3A2007-07-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 1 tweets (1 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,337 Got 1 tweets (1 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 19 tweets for fluoxetine%20since%3A2008-03-01%20until%3A2008-10-25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,438 Got 19 tweets for fluoxetine%20since%3A2008-03-01%20until%3A2008-10-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets (19 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,440 Got 20 tweets (19 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,557 Got 20 tweets for fluoxetine%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,595 Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2014-01-05%20until%3A2014-08-30.\n",
      "INFO: Got 20 tweets for fluoxetine%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,637 Got 20 tweets for fluoxetine%20since%3A2014-01-05%20until%3A2014-08-30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2011-06-01%20until%3A2012-01-24.\n",
      "INFO: Got 60 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,671 Got 60 tweets (20 new).\n",
      "2019-03-19 18:20:19,639 Got 20 tweets for fluoxetine%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 80 tweets (20 new).\n",
      "INFO: Got 20 tweets for fluoxetine%20since%3A2013-05-13%20until%3A2014-01-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,677 Got 80 tweets (20 new).\n",
      "2019-03-19 18:20:19,650 Got 20 tweets for fluoxetine%20since%3A2011-06-01%20until%3A2012-01-24.\n",
      "2019-03-19 18:20:19,669 Got 20 tweets for fluoxetine%20since%3A2013-05-13%20until%3A2014-01-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 100 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,704 Got 100 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 10 tweets for fluoxetine%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,687 Got 10 tweets for fluoxetine%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 110 tweets (10 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,725 Got 110 tweets (10 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 130 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,727 Got 130 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2016-08-11%20until%3A2017-04-06.\n",
      "INFO: Got 20 tweets for fluoxetine%20since%3A2017-11-29%20until%3A2018-07-24.\n",
      "INFO: Got 20 tweets for fluoxetine%20since%3A2008-10-25%20until%3A2009-06-19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,801 Got 20 tweets for fluoxetine%20since%3A2016-08-11%20until%3A2017-04-06.\n",
      "2019-03-19 18:20:19,819 Got 20 tweets for fluoxetine%20since%3A2017-11-29%20until%3A2018-07-24.\n",
      "2019-03-19 18:20:19,821 Got 20 tweets for fluoxetine%20since%3A2008-10-25%20until%3A2009-06-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 150 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,827 Got 150 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 170 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,829 Got 170 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 190 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,833 Got 190 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2015-04-25%20until%3A2015-12-18.\n",
      "INFO: Got 20 tweets for fluoxetine%20since%3A2010-02-11%20until%3A2010-10-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,826 Got 20 tweets for fluoxetine%20since%3A2015-04-25%20until%3A2015-12-18.\n",
      "2019-03-19 18:20:19,838 Got 20 tweets for fluoxetine%20since%3A2010-02-11%20until%3A2010-10-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 210 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,843 Got 210 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 230 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,844 Got 230 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,849 Got 20 tweets for fluoxetine%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 250 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,856 Got 250 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2012-01-24%20until%3A2012-09-18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,878 Got 20 tweets for fluoxetine%20since%3A2012-01-24%20until%3A2012-09-18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 270 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,880 Got 270 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2010-10-07%20until%3A2011-06-01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,887 Got 20 tweets for fluoxetine%20since%3A2010-10-07%20until%3A2011-06-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 290 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,889 Got 290 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,903 Got 20 tweets for fluoxetine%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 310 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,906 Got 310 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2012-09-18%20until%3A2013-05-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,909 Got 20 tweets for fluoxetine%20since%3A2012-09-18%20until%3A2013-05-13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 330 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,911 Got 330 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for fluoxetine%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,922 Got 20 tweets for fluoxetine%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 350 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:20:19,924 Got 350 tweets (20 new).\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detectTwitter(tagger, product):\n",
    "    tw = TwitterWatch(tagger)\n",
    "    tweets = tw.query(product)\n",
    "    tweetsdict = list(map(lambda t: \n",
    "    {\n",
    "        'Name'          : t.fullname, \n",
    "        'SourceEntity'  : t.source_entity, \n",
    "        'TextEntity'    : t.text_entities,\n",
    "        'Text'          : t.text, \n",
    "        'hasDrug'       : t.hasDrug,\n",
    "        'hasSymptom'    : t.hasSymptom\n",
    "    }, tweets))\n",
    "    df = pd.DataFrame.from_dict(tweetsdict)\n",
    "    df = df[['Name', 'SourceEntity', 'TextEntity', 'hasDrug', 'hasSymptom', 'Text']]\n",
    "    df = df.sort_values(['SourceEntity', 'hasDrug', 'hasSymptom'])\n",
    "    df.to_excel(f\"output_{product}.xlsx\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['Mylan since:2006-03-21 until:2006-11-13', 'Mylan since:2006-11-13 until:2007-07-08', 'Mylan since:2007-07-08 until:2008-03-01', 'Mylan since:2008-03-01 until:2008-10-25', 'Mylan since:2008-10-25 until:2009-06-19', 'Mylan since:2009-06-19 until:2010-02-11', 'Mylan since:2010-02-11 until:2010-10-07', 'Mylan since:2010-10-07 until:2011-06-01', 'Mylan since:2011-06-01 until:2012-01-24', 'Mylan since:2012-01-24 until:2012-09-18', 'Mylan since:2012-09-18 until:2013-05-13', 'Mylan since:2013-05-13 until:2014-01-05', 'Mylan since:2014-01-05 until:2014-08-30', 'Mylan since:2014-08-30 until:2015-04-25', 'Mylan since:2015-04-25 until:2015-12-18', 'Mylan since:2015-12-18 until:2016-08-11', 'Mylan since:2016-08-11 until:2017-04-06', 'Mylan since:2017-04-06 until:2017-11-29', 'Mylan since:2017-11-29 until:2018-07-24', 'Mylan since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,939 queries: ['Mylan since:2006-03-21 until:2006-11-13', 'Mylan since:2006-11-13 until:2007-07-08', 'Mylan since:2007-07-08 until:2008-03-01', 'Mylan since:2008-03-01 until:2008-10-25', 'Mylan since:2008-10-25 until:2009-06-19', 'Mylan since:2009-06-19 until:2010-02-11', 'Mylan since:2010-02-11 until:2010-10-07', 'Mylan since:2010-10-07 until:2011-06-01', 'Mylan since:2011-06-01 until:2012-01-24', 'Mylan since:2012-01-24 until:2012-09-18', 'Mylan since:2012-09-18 until:2013-05-13', 'Mylan since:2013-05-13 until:2014-01-05', 'Mylan since:2014-01-05 until:2014-08-30', 'Mylan since:2014-08-30 until:2015-04-25', 'Mylan since:2015-04-25 until:2015-12-18', 'Mylan since:2015-12-18 until:2016-08-11', 'Mylan since:2016-08-11 until:2017-04-06', 'Mylan since:2017-04-06 until:2017-11-29', 'Mylan since:2017-11-29 until:2018-07-24', 'Mylan since:2018-07-24 until:2019-03-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Mylan since:2008-10-25 until:2009-06-19\n",
      "INFO: Querying Mylan since:2007-07-08 until:2008-03-01\n",
      "INFO: Querying Mylan since:2009-06-19 until:2010-02-11\n",
      "INFO: Querying Mylan since:2006-03-21 until:2006-11-13\n",
      "INFO: Querying Mylan since:2008-03-01 until:2008-10-25\n",
      "INFO: Querying Mylan since:2006-11-13 until:2007-07-08\n",
      "INFO: Querying Mylan since:2012-01-24 until:2012-09-18\n",
      "INFO: Querying Mylan since:2014-08-30 until:2015-04-25\n",
      "INFO: Querying Mylan since:2011-06-01 until:2012-01-24\n",
      "INFO: Querying Mylan since:2013-05-13 until:2014-01-05\n",
      "INFO: Querying Mylan since:2016-08-11 until:2017-04-06\n",
      "INFO: Querying Mylan since:2015-12-18 until:2016-08-11\n",
      "INFO: Querying Mylan since:2014-01-05 until:2014-08-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,941 Querying Mylan since:2008-10-25 until:2009-06-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Mylan since:2017-04-06 until:2017-11-29\n",
      "INFO: Querying Mylan since:2012-09-18 until:2013-05-13\n",
      "INFO: Querying Mylan since:2010-02-11 until:2010-10-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,941 Querying Mylan since:2006-03-21 until:2006-11-13\n",
      "2019-03-19 18:56:53,941 Querying Mylan since:2007-07-08 until:2008-03-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Mylan since:2010-10-07 until:2011-06-01\n",
      "INFO: Querying Mylan since:2015-04-25 until:2015-12-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,941 Querying Mylan since:2006-11-13 until:2007-07-08\n",
      "2019-03-19 18:56:53,945 Querying Mylan since:2014-08-30 until:2015-04-25\n",
      "2019-03-19 18:56:53,943 Querying Mylan since:2013-05-13 until:2014-01-05\n",
      "2019-03-19 18:56:53,944 Querying Mylan since:2014-01-05 until:2014-08-30\n",
      "2019-03-19 18:56:53,943 Querying Mylan since:2010-02-11 until:2010-10-07\n",
      "2019-03-19 18:56:53,945 Querying Mylan since:2016-08-11 until:2017-04-06\n",
      "2019-03-19 18:56:53,945 Querying Mylan since:2015-12-18 until:2016-08-11\n",
      "2019-03-19 18:56:53,942 Querying Mylan since:2009-06-19 until:2010-02-11\n",
      "2019-03-19 18:56:53,945 Querying Mylan since:2017-04-06 until:2017-11-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Mylan since:2017-11-29 until:2018-07-24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,943 Querying Mylan since:2012-09-18 until:2013-05-13\n",
      "2019-03-19 18:56:53,943 Querying Mylan since:2010-10-07 until:2011-06-01\n",
      "2019-03-19 18:56:53,943 Querying Mylan since:2012-01-24 until:2012-09-18\n",
      "2019-03-19 18:56:53,945 Querying Mylan since:2015-04-25 until:2015-12-18\n",
      "2019-03-19 18:56:53,943 Querying Mylan since:2011-06-01 until:2012-01-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Querying Mylan since:2018-07-24 until:2019-03-19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:53,947 Querying Mylan since:2017-11-29 until:2018-07-24\n",
      "2019-03-19 18:56:53,941 Querying Mylan since:2008-03-01 until:2008-10-25\n",
      "2019-03-19 18:56:53,946 Querying Mylan since:2018-07-24 until:2019-03-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets for Mylan%20since%3A2006-03-21%20until%3A2006-11-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,549 Got 0 tweets for Mylan%20since%3A2006-03-21%20until%3A2006-11-13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,583 Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2016-08-11%20until%3A2017-04-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,686 Got 20 tweets for Mylan%20since%3A2016-08-11%20until%3A2017-04-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,716 Got 20 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2008-03-01%20until%3A2008-10-25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,715 Got 20 tweets for Mylan%20since%3A2008-03-01%20until%3A2008-10-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,754 Got 40 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,758 Got 20 tweets for Mylan%20since%3A2014-08-30%20until%3A2015-04-25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 60 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,783 Got 60 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,770 Got 20 tweets for Mylan%20since%3A2017-04-06%20until%3A2017-11-29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 80 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,819 Got 80 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 7 tweets for Mylan%20since%3A2006-11-13%20until%3A2007-07-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,854 Got 7 tweets for Mylan%20since%3A2006-11-13%20until%3A2007-07-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 87 tweets (7 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,891 Got 87 tweets (7 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2015-04-25%20until%3A2015-12-18.\n",
      "INFO: Got 20 tweets for Mylan%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,875 Got 20 tweets for Mylan%20since%3A2018-07-24%20until%3A2019-03-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2013-05-13%20until%3A2014-01-05.\n",
      "INFO: Got 107 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,919 Got 107 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,897 Got 20 tweets for Mylan%20since%3A2015-04-25%20until%3A2015-12-18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 127 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,927 Got 127 tweets (20 new).\n",
      "2019-03-19 18:56:54,917 Got 20 tweets for Mylan%20since%3A2013-05-13%20until%3A2014-01-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 147 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,944 Got 147 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2014-01-05%20until%3A2014-08-30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,912 Got 20 tweets for Mylan%20since%3A2015-12-18%20until%3A2016-08-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 167 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,955 Got 167 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2010-02-11%20until%3A2010-10-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,953 Got 20 tweets for Mylan%20since%3A2014-01-05%20until%3A2014-08-30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 187 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,964 Got 187 tweets (20 new).\n",
      "2019-03-19 18:56:54,955 Got 20 tweets for Mylan%20since%3A2010-02-11%20until%3A2010-10-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 207 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,979 Got 207 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2011-06-01%20until%3A2012-01-24.\n",
      "INFO: Got 20 tweets for Mylan%20since%3A2017-11-29%20until%3A2018-07-24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,984 Got 20 tweets for Mylan%20since%3A2017-11-29%20until%3A2018-07-24.\n",
      "2019-03-19 18:56:54,975 Got 20 tweets for Mylan%20since%3A2011-06-01%20until%3A2012-01-24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2012-09-18%20until%3A2013-05-13.\n",
      "INFO: Got 227 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,996 Got 20 tweets for Mylan%20since%3A2012-09-18%20until%3A2013-05-13.\n",
      "2019-03-19 18:56:54,997 Got 227 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 247 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:54,999 Got 247 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 267 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,000 Got 267 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,002 Got 20 tweets for Mylan%20since%3A2007-07-08%20until%3A2008-03-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 287 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,005 Got 287 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,014 Got 20 tweets for Mylan%20since%3A2009-06-19%20until%3A2010-02-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 307 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,015 Got 307 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2008-10-25%20until%3A2009-06-19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,021 Got 20 tweets for Mylan%20since%3A2008-10-25%20until%3A2009-06-19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 327 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,023 Got 327 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2010-10-07%20until%3A2011-06-01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,027 Got 20 tweets for Mylan%20since%3A2010-10-07%20until%3A2011-06-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 347 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,029 Got 347 tweets (20 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 20 tweets for Mylan%20since%3A2012-01-24%20until%3A2012-09-18.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,029 Got 20 tweets for Mylan%20since%3A2012-01-24%20until%3A2012-09-18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 367 tweets (20 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 18:56:55,031 Got 367 tweets (20 new).\n"
     ]
    }
   ],
   "source": [
    "detectTwitter(tagger, 'Mylan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
